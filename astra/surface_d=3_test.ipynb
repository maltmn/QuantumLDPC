{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c83a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from panqec.codes import surface_2d\n",
    "from panqec.error_models import PauliErrorModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "#import tool\n",
    "from panq_functions import GNNDecoder, collate, fraction_of_solved_puzzles, compute_accuracy, logical_error_rate, \\\n",
    "    surface_code_edges, generate_syndrome_error_volume, adapt_trainset, ler_loss, save_model, load_model\n",
    "\n",
    "from ldpc.mod2 import *\n",
    "\n",
    "# ------------------------------------------\n",
    "# Device selection: MPS (Mac GPU) → CPU\n",
    "# ------------------------------------------\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    use_amp = False  # AMP on MPS is unstable\n",
    "    amp_data_type = torch.float32\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_amp = False\n",
    "    amp_data_type = torch.float32\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326e6604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN hyperparameters: iters: 3 node features: 50 edge features: 50\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Parameters (surface code d=3)\n",
    "# ============================================\n",
    "d = 3\n",
    "error_model_name = \"DP\"  # depolarizing\n",
    "\n",
    "# DEPOLARIZING ERROR MODEL\n",
    "error_model = PauliErrorModel(0.34, 0.32, 0.34)\n",
    "\n",
    "# GNN Hyperparameters\n",
    "n_node_inputs = 4\n",
    "n_node_outputs = 4\n",
    "n_iters = 3\n",
    "n_node_features = 50\n",
    "n_edge_features = 50\n",
    "\n",
    "len_test_set = 500\n",
    "test_err_rate = 0.10\n",
    "\n",
    "len_train_set = 5000\n",
    "max_train_err_rate = 0.15\n",
    "\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "msg_net_size = 512\n",
    "msg_net_dropout_p = 0.05\n",
    "gru_dropout_p = 0.05\n",
    "\n",
    "print(\"GNN hyperparameters:\",\n",
    "      \"iters:\", n_iters, \"node features:\", n_node_features,\n",
    "      \"edge features:\", n_edge_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fd90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters: 618778\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Build Surface Code + GNN\n",
    "# ============================================\n",
    "\n",
    "dist = d\n",
    "code = surface_2d.RotatedPlanar2DCode(dist)\n",
    "\n",
    "gnn = GNNDecoder(\n",
    "    dist=dist,\n",
    "    n_node_inputs=n_node_inputs,\n",
    "    n_node_outputs=n_node_outputs,\n",
    "    n_iters=n_iters,\n",
    "    n_node_features=n_node_features,\n",
    "    n_edge_features=n_edge_features,\n",
    "    msg_net_size=msg_net_size,\n",
    "    msg_net_dropout_p=msg_net_dropout_p,\n",
    "    gru_dropout_p=gru_dropout_p,\n",
    ")\n",
    "\n",
    "gnn.to(device)\n",
    "\n",
    "# Tanner graph edges\n",
    "src, tgt = surface_code_edges(code)\n",
    "GNNDecoder.surface_code_edges = (\n",
    "    torch.LongTensor(src),\n",
    "    torch.LongTensor(tgt),\n",
    ")\n",
    "\n",
    "# -------------------------------------------\n",
    "# Degeneracy nullspaces (using ldpc.mod2)\n",
    "# -------------------------------------------\n",
    "\n",
    "hx_null = nullspace(code.Hx.toarray())\n",
    "hz_null = nullspace(code.Hz.toarray())\n",
    "\n",
    "# --- convert sparse → dense ---\n",
    "if not isinstance(hx_null, np.ndarray):\n",
    "    hx_null = hx_null.toarray()\n",
    "\n",
    "if not isinstance(hz_null, np.ndarray):\n",
    "    hz_null = hz_null.toarray()\n",
    "\n",
    "GNNDecoder.hxperp = torch.tensor(hx_null, dtype=torch.float32, device=device)\n",
    "GNNDecoder.hzperp = torch.tensor(hz_null, dtype=torch.float32, device=device)\n",
    "\n",
    "GNNDecoder.device = device\n",
    "\n",
    "print(\"Total model parameters:\", sum(p.numel() for p in gnn.parameters()))\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Create Test Dataset\n",
    "# ============================================\n",
    "\n",
    "testset = adapt_trainset(\n",
    "    generate_syndrome_error_volume(\n",
    "        code,\n",
    "        error_model=error_model,\n",
    "        p=test_err_rate,\n",
    "        batch_size=len_test_set,\n",
    "        for_training=False,\n",
    "    ),\n",
    "    code,\n",
    "    num_classes=n_node_inputs,\n",
    "    for_training=False,\n",
    ")\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=512, collate_fn=collate, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d51e321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Training Setup\n",
    "# ============================================\n",
    "\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "\n",
    "fnamenew = (\n",
    "    f\"trained_models/d{dist}_{error_model_name}_\"\n",
    "    f\"{n_iters}_{n_node_features}_{n_edge_features}_\"\n",
    ")\n",
    "\n",
    "optimizer = optim.AdamW(gnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "le_rates = np.zeros((epochs, 5))\n",
    "start_time = time.time()\n",
    "\n",
    "size = 2 * dist**2 - 1\n",
    "error_index = dist**2 - 1\n",
    "\n",
    "min_ler_tot = 1.0\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Prepare training set (regenerated each epoch)\n",
    "# ============================================\n",
    "\n",
    "trainset = adapt_trainset(\n",
    "    generate_syndrome_error_volume(\n",
    "        code,\n",
    "        error_model=error_model,\n",
    "        p=max_train_err_rate,\n",
    "        batch_size=len_train_set,\n",
    "    ),\n",
    "    code,\n",
    "    num_classes=n_node_inputs,\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, collate_fn=collate, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d2e329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 1 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 2 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 3 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 4 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 5 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 6 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 7 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 8 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 9 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 10 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 11 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 12 | LER_tot=1.00000 | LERx=0.87800 | LERz=0.62000\n",
      "Epoch 13 | LER_tot=0.99800 | LERx=0.87800 | LERz=0.98000\n",
      "Model saved to trained_models/d3_DP_3_50_50_gnn_best_13.pth 0.878_0.98.\n",
      "Epoch 14 | LER_tot=0.99800 | LERx=0.87800 | LERz=0.98000\n",
      "Epoch 15 | LER_tot=0.99800 | LERx=0.87800 | LERz=0.98000\n",
      "Epoch 16 | LER_tot=1.00000 | LERx=0.32200 | LERz=1.00000\n",
      "Epoch 17 | LER_tot=1.00000 | LERx=0.32200 | LERz=1.00000\n",
      "Epoch 18 | LER_tot=1.00000 | LERx=0.32200 | LERz=1.00000\n",
      "Epoch 19 | LER_tot=1.00000 | LERx=0.32200 | LERz=1.00000\n",
      "Epoch 20 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Model saved to trained_models/d3_DP_3_50_50_gnn_best_20.pth 0.322_0.62.\n",
      "Epoch 21 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 22 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 23 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 24 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 25 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 26 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 27 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 28 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 29 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 30 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 31 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 32 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 33 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 34 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 35 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 36 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 37 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 38 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 39 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 40 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 41 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 42 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 43 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 44 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 45 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 46 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 47 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 48 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 49 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 50 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 51 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 52 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 53 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 54 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 55 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 56 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 57 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 58 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 59 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 60 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 61 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 62 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 63 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 64 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 65 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 66 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 67 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 68 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 69 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 70 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 71 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 72 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 73 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 74 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 75 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 76 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 77 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 78 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 79 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 80 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 81 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 82 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 83 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 84 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 85 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 86 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 87 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 88 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 89 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 90 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 91 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 92 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 93 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 94 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 95 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 96 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 97 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 98 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 99 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 100 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 101 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 102 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 103 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 104 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 105 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 106 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 107 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 108 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 109 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 110 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 111 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 112 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 113 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 114 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 115 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 116 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 117 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 118 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 119 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 120 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 121 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 122 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 123 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 124 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 125 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 126 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 127 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 128 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 129 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 130 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 131 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 132 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 133 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 134 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 135 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 136 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 137 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 138 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 139 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 140 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 141 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 142 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 143 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 144 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 145 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 146 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 147 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 148 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 149 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 150 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 151 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 152 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 153 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 154 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 155 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 156 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 157 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 158 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 159 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 160 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 161 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 162 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 163 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 164 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 165 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 166 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 167 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 168 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 169 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 170 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 171 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 172 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 173 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 174 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 175 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 176 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 177 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 178 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 179 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 180 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 181 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 182 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 183 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 184 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 185 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 186 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 187 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 188 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 189 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 190 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 191 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 192 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 193 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 194 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 195 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 196 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 197 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 198 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Epoch 199 | LER_tot=0.62000 | LERx=0.32200 | LERz=0.62000\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Main Training Loop\n",
    "# ============================================\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    gnn.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for inputs, targets, src_ids, dst_ids in trainloader:\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        src_ids, dst_ids = src_ids.to(device), dst_ids.to(device)\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "\n",
    "    # ---- Forward pass (NO autocast on MPS) ----\n",
    "    if device.type == \"cuda\" and use_amp:\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=amp_data_type):\n",
    "            outputs = gnn(inputs, src_ids, dst_ids)\n",
    "    else:\n",
    "        # MPS or CPU — no autocast\n",
    "        outputs = gnn(inputs, src_ids, dst_ids)\n",
    "\n",
    "    loss = 0.0\n",
    "    for out in outputs:\n",
    "        eloss = criterion(\n",
    "            out.view(-1, size, n_node_inputs)[:, error_index:].reshape(-1, n_node_inputs),\n",
    "            targets.view(-1, size)[:, error_index:].flatten(),\n",
    "        )\n",
    "        sloss = criterion(\n",
    "            out.view(-1, size, n_node_inputs)[:, :error_index].reshape(-1, n_node_inputs),\n",
    "            targets.view(-1, size)[:, :error_index].flatten(),\n",
    "        )\n",
    "        loss = loss + ler_loss(out, targets, code) + sloss + eloss\n",
    "\n",
    "    loss = loss / outputs.shape[0]\n",
    "\n",
    "    if device.type == \"cuda\" and use_amp:\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    epoch_losses.append(loss.item())\n",
    "\n",
    "    # ------------- END BATCH LOOP -------------\n",
    "\n",
    "    # Evaluate every epoch\n",
    "    gnn.eval()\n",
    "    with torch.no_grad():\n",
    "        fraction = fraction_of_solved_puzzles(gnn, testloader, code)\n",
    "        test_loss = compute_accuracy(gnn, testloader, code)\n",
    "        lerx, lerz, ler_tot = logical_error_rate(gnn, testloader, code)\n",
    "\n",
    "    le_rates[epoch] = [lerx, lerz, ler_tot, test_loss, np.mean(epoch_losses)]\n",
    "\n",
    "    print(f\"Epoch {epoch} | LER_tot={ler_tot:.5f} | LERx={lerx:.5f} | LERz={lerz:.5f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if ler_tot < min_ler_tot:\n",
    "        min_ler_tot = ler_tot\n",
    "        save_model(gnn, fnamenew + f\"gnn_best_{epoch}.pth {lerx}_{lerz}\", confirm=False)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        np.save(fnamenew + \"training_progress.npy\", le_rates)\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8fda4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra-m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
