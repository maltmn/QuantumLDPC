{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326e6604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "(4, 9)\n",
      "Using device: mps\n",
      "n_iters: 3 n_node_outputs: 4 n_node_features: 50 n_edge_features: 50\n",
      "msg_net_size: 512 msg_net_dropout_p: 0.05 gru_dropout_p: 0.05\n",
      "learning rate: 0.0001 weight decay: 0.0001 len train set: 1000 max train err rate: 0.15 len test set: 100 test err rate: 0.05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from panqec.codes import surface_2d\n",
    "from panqec.error_models import PauliErrorModel\n",
    "from panqec.decoders import BeliefPropagationOSDDecoder, MatchingDecoder\n",
    "from fnn_model import FNNDecoder\n",
    "from fnn_data import make_fnn_dataset\n",
    "from train_fnn import train_fnn_for_distance\n",
    "\n",
    "\n",
    "from ldpc.mod2 import nullspace\n",
    "\n",
    "from panq_functions import (\n",
    "    GNNDecoder,\n",
    "    collate,\n",
    "    generate_syndrome_error_volume,\n",
    "    adapt_trainset,\n",
    "    logical_error_rate,\n",
    "    fraction_of_solved_puzzles,\n",
    "    surface_code_edges,\n",
    "    load_model,\n",
    "    save_model\n",
    ")\n",
    "\n",
    "# ------------------------------------------\n",
    "# Device selection: MPS (Mac GPU)  â†’ CPU\n",
    "# ------------------------------------------\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ==========================================================\n",
    "# Parameters (must match training!)\n",
    "# ==========================================================\n",
    "\n",
    "error_model_name = \"DP\"\n",
    "if error_model_name == \"X\":\n",
    "    error_model = PauliErrorModel(1, 0.0, 0)\n",
    "elif error_model_name == \"Z\":\n",
    "    error_model = PauliErrorModel(0, 0.0, 1)\n",
    "elif error_model_name == \"XZ\":\n",
    "    error_model = PauliErrorModel(0.5, 0.0, 0.5)\n",
    "elif error_model_name == \"DP\":\n",
    "    error_model = PauliErrorModel(0.34, 0.32, 0.34)\n",
    "\n",
    "n_node_inputs = 4\n",
    "n_node_outputs = 4\n",
    "n_iters = 3\n",
    "n_node_features = 50\n",
    "n_edge_features = 50\n",
    "\n",
    "len_test_set = 100        # tiny monitoring set (like original code)\n",
    "test_err_rate = 0.05\n",
    "\n",
    "len_train_set = len_test_set * 10\n",
    "max_train_err_rate = 0.15\n",
    "\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "msg_net_size = 512\n",
    "msg_net_dropout_p = 0.05\n",
    "gru_dropout_p = 0.05\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "distances = [3, 5, 7, 9, 11]\n",
    "\n",
    "print(\"n_iters:\", n_iters,\n",
    "      \"n_node_outputs:\", n_node_outputs,\n",
    "      \"n_node_features:\", n_node_features,\n",
    "      \"n_edge_features:\", n_edge_features)\n",
    "print(\"msg_net_size:\", msg_net_size,\n",
    "      \"msg_net_dropout_p:\", msg_net_dropout_p,\n",
    "      \"gru_dropout_p:\", gru_dropout_p)\n",
    "print(\"learning rate:\", lr,\n",
    "      \"weight decay:\", weight_decay,\n",
    "      \"len train set:\", len_train_set,\n",
    "      \"max train err rate:\", max_train_err_rate,\n",
    "      \"len test set:\", len_test_set,\n",
    "      \"test err rate:\", test_err_rate)\n",
    "\n",
    "os.makedirs(\"trained_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebe8fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================\n",
      "Training distance 3\n",
      "=========================================\n",
      "Total params: 618778\n",
      "epoch, lr, wd, fract. corr. synd, LER_X, LER_Z, LER_tot, test loss, train loss, train time\n",
      "0 0.0001 0.0001 0.68 0.14 0.32 0.32 3.086498975753784 3.211421012878418 4.49920392036438\n",
      "Model saved to trained_models/d3_DP_best.pth.\n",
      "1 0.0001 0.0001 0.68 0.14 0.32 0.32 2.859165906906128 3.023329973220825 5.198629140853882\n",
      "2 0.0001 0.0001 0.68 0.14 0.32 0.32 2.6998727321624756 2.862102508544922 5.8637590408325195\n",
      "3 0.0001 0.0001 0.68 0.14 0.32 0.32 2.560600996017456 2.737417221069336 6.512370824813843\n",
      "4 0.0001 0.0001 0.68 0.14 0.32 0.32 2.4429099559783936 2.642266035079956 7.150183916091919\n",
      "5 0.0001 0.0001 0.68 0.14 0.32 0.32 2.358523368835449 2.561842918395996 7.804999113082886\n",
      "6 0.0001 0.0001 0.68 0.21 1.0 1.0 2.2933504581451416 2.504667282104492 8.433401107788086\n",
      "7 0.0001 0.0001 0.68 0.14 0.32 0.32 2.2406599521636963 2.4602503776550293 9.07447099685669\n",
      "8 0.0001 0.0001 0.68 0.14 0.32 0.32 2.210193634033203 2.431593418121338 9.712883949279785\n",
      "9 0.0001 0.0001 0.68 0.14 0.32 0.32 2.201981782913208 2.4057719707489014 10.348609924316406\n",
      "10 0.0001 0.0001 0.68 0.14 0.32 0.32 2.164076089859009 2.3783345222473145 10.998564958572388\n",
      "11 0.0001 0.0001 0.68 0.14 0.32 0.32 2.122457265853882 2.341409683227539 11.664263010025024\n",
      "12 0.0001 0.0001 0.68 0.14 0.32 0.32 2.0875585079193115 2.30497407913208 12.334513187408447\n",
      "13 0.0001 0.0001 0.68 0.14 0.32 0.32 2.0751473903656006 2.276749610900879 13.018845796585083\n",
      "14 0.0001 0.0001 0.68 0.14 0.32 0.32 2.0506346225738525 2.2606420516967773 13.701168060302734\n",
      "15 0.0001 0.0001 0.68 0.14 0.32 0.32 2.0463545322418213 2.2456486225128174 14.420491933822632\n",
      "16 0.0001 0.0001 0.68 0.14 0.32 0.32 2.054652452468872 2.2343976497650146 15.097799062728882\n",
      "17 0.0001 0.0001 0.68 0.14 0.32 0.32 2.0270497798919678 2.226668357849121 15.756373882293701\n",
      "18 0.0001 0.0001 0.68 0.14 0.32 0.32 2.024939775466919 2.218352794647217 16.411479949951172\n",
      "19 0.0001 0.0001 0.68 0.14 0.32 0.32 2.01438570022583 2.2107839584350586 17.080403804779053\n",
      "20 0.0001 0.0001 0.68 0.14 0.32 0.32 2.016720771789551 2.202287435531616 17.76293396949768\n",
      "21 0.0001 0.0001 0.68 0.14 0.32 0.32 2.002877950668335 2.197232723236084 18.4294331073761\n",
      "22 0.0001 0.0001 0.68 0.14 0.32 0.32 1.9964081048965454 2.1915407180786133 19.11346983909607\n",
      "23 0.0001 0.0001 0.68 0.14 0.32 0.32 1.9941591024398804 2.183851718902588 19.786813974380493\n",
      "24 0.0001 0.0001 0.68 0.14 0.32 0.32 1.9904723167419434 2.177658796310425 20.463325023651123\n",
      "25 0.0001 0.0001 0.68 0.14 0.3 0.3 1.9840630292892456 2.1708993911743164 21.143378019332886\n",
      "Model saved to trained_models/d3_DP_best.pth.\n",
      "26 0.0001 0.0001 0.68 0.12 0.26 0.26 1.979270577430725 2.1649327278137207 21.811280965805054\n",
      "Model saved to trained_models/d3_DP_best.pth.\n",
      "27 0.0001 0.0001 0.68 0.14 0.28 0.28 1.9700759649276733 2.1573338508605957 22.484626054763794\n",
      "28 0.0001 0.0001 0.68 0.14 0.28 0.28 1.962149977684021 2.1503636837005615 23.143374919891357\n",
      "29 0.0001 0.0001 0.68 0.14 0.28 0.28 1.9517804384231567 2.1408426761627197 23.814160108566284\n",
      "30 0.0001 0.0001 0.68 0.14 0.28 0.28 1.9473729133605957 2.1321170330047607 24.481159210205078\n",
      "31 0.0001 0.0001 0.68 0.15 0.28 0.28 1.9396511316299438 2.1237545013427734 25.173686981201172\n",
      "32 0.0001 0.0001 0.68 0.15 0.28 0.28 1.9334330558776855 2.1140592098236084 25.83555507659912\n",
      "33 0.0001 0.0001 0.68 0.15 0.28 0.28 1.9278563261032104 2.1028366088867188 26.492367029190063\n",
      "34 0.0001 0.0001 0.68 0.15 0.28 0.28 1.9179930686950684 2.093562126159668 27.169858932495117\n",
      "35 0.0001 0.0001 0.68 0.15 0.28 0.28 1.9111870527267456 2.0829861164093018 27.834515810012817\n",
      "36 0.0001 0.0001 0.68 0.15 0.28 0.28 1.9016976356506348 2.0721118450164795 28.498361110687256\n",
      "37 0.0001 0.0001 0.68 0.15 0.28 0.28 1.8883873224258423 2.05962872505188 29.172924757003784\n",
      "38 0.0001 0.0001 0.68 0.15 0.28 0.28 1.880898118019104 2.0493712425231934 29.844463109970093\n",
      "39 0.0001 0.0001 0.68 0.15 0.28 0.28 1.8758716583251953 2.0400428771972656 30.51373314857483\n",
      "40 0.0001 0.0001 0.68 0.15 0.28 0.28 1.8684800863265991 2.032888174057007 31.163959980010986\n",
      "41 0.0001 0.0001 0.68 0.15 0.28 0.28 1.8710397481918335 2.0260794162750244 31.81095600128174\n",
      "42 0.0001 0.0001 0.68 0.14 0.28 0.28 1.857418417930603 2.01861572265625 32.458165884017944\n",
      "43 0.0001 0.0001 0.68 0.15 0.28 0.28 1.8522008657455444 2.011127233505249 33.11021590232849\n",
      "44 0.0001 0.0001 0.68 0.15 0.28 0.28 1.8487557172775269 2.004119396209717 33.76733684539795\n",
      "45 0.0001 0.0001 0.75 0.15 0.28 0.28 1.8427047729492188 1.997956395149231 34.42939496040344\n",
      "46 0.0001 0.0001 0.77 0.15 0.28 0.28 1.8378639221191406 1.9915825128555298 35.113775968551636\n",
      "47 0.0001 0.0001 0.75 0.15 0.29 0.29 1.8345307111740112 1.9863272905349731 35.75718688964844\n",
      "48 0.0001 0.0001 0.79 0.15 0.29 0.29 1.830784797668457 1.9805002212524414 36.41017699241638\n",
      "49 0.0001 0.0001 0.79 0.13 0.29 0.29 1.8277324438095093 1.9772778749465942 37.06512999534607\n",
      "50 0.0001 0.0001 0.9 0.12 0.29 0.29 1.8232674598693848 1.969733715057373 37.7317271232605\n",
      "51 0.0001 0.0001 0.9 0.12 0.29 0.29 1.8180841207504272 1.9648677110671997 38.38749408721924\n",
      "52 0.0001 0.0001 1.0 0.12 0.29 0.29 1.8153549432754517 1.96094810962677 39.054497957229614\n",
      "53 0.0001 0.0001 1.0 0.11 0.31 0.31 1.8107715845108032 1.95673668384552 39.720935106277466\n",
      "54 0.0001 0.0001 1.0 0.12 0.29 0.29 1.8072338104248047 1.9508787393569946 40.405869007110596\n",
      "55 0.0001 0.0001 1.0 0.11 0.31 0.31 1.8037667274475098 1.9468462467193604 41.07491707801819\n",
      "56 0.0001 0.0001 1.0 0.13 0.32 0.32 1.8058795928955078 1.9428421258926392 41.73454189300537\n",
      "57 0.0001 0.0001 1.0 0.13 0.32 0.32 1.7979270219802856 1.9376734495162964 42.397138833999634\n",
      "58 0.0001 0.0001 1.0 0.13 0.32 0.32 1.7945502996444702 1.9320826530456543 43.067349910736084\n",
      "59 0.0001 0.0001 1.0 0.13 0.32 0.32 1.7863727807998657 1.9248875379562378 43.714558839797974\n",
      "60 0.0001 0.0001 1.0 0.13 0.32 0.32 1.7759718894958496 1.9150826930999756 44.370595932006836\n",
      "61 0.0001 0.0001 1.0 0.13 0.32 0.32 1.771116852760315 1.90879487991333 45.07475996017456\n",
      "62 0.0001 0.0001 1.0 0.13 0.32 0.32 1.7649627923965454 1.9021955728530884 45.73804688453674\n",
      "63 0.0001 0.0001 1.0 0.12 0.29 0.29 1.759278416633606 1.8972986936569214 46.41026711463928\n",
      "64 0.0001 0.0001 1.0 0.11 0.28 0.28 1.7570255994796753 1.8930859565734863 47.213099002838135\n",
      "65 0.0001 0.0001 1.0 0.13 0.29 0.29 1.7522687911987305 1.886765480041504 47.86527895927429\n",
      "66 0.0001 0.0001 1.0 0.13 0.29 0.29 1.7483692169189453 1.8829929828643799 48.54588794708252\n",
      "67 0.0001 0.0001 1.0 0.13 0.29 0.29 1.7471609115600586 1.879063606262207 49.1903920173645\n",
      "68 0.0001 0.0001 1.0 0.13 0.29 0.29 1.7402743101119995 1.8740253448486328 49.850269079208374\n",
      "69 0.0001 0.0001 1.0 0.12 0.29 0.29 1.7363613843917847 1.8696205615997314 50.5006058216095\n",
      "70 0.0001 0.0001 1.0 0.12 0.29 0.29 1.7362035512924194 1.8662190437316895 51.15253496170044\n",
      "71 0.0001 0.0001 1.0 0.12 0.28 0.28 1.730878472328186 1.8626184463500977 51.790924072265625\n",
      "72 0.0001 0.0001 1.0 0.12 0.28 0.28 1.7276649475097656 1.8585546016693115 52.43130302429199\n",
      "73 0.0001 0.0001 1.0 0.12 0.29 0.29 1.7240252494812012 1.8548818826675415 53.124577045440674\n",
      "74 0.0001 0.0001 1.0 0.12 0.28 0.28 1.7206993103027344 1.85015869140625 53.77975606918335\n",
      "75 0.0001 0.0001 1.0 0.11 0.26 0.26 1.7175017595291138 1.847837209701538 54.433040142059326\n",
      "76 0.0001 0.0001 1.0 0.13 0.28 0.28 1.7158914804458618 1.8442785739898682 55.06506299972534\n",
      "77 0.0001 0.0001 1.0 0.13 0.28 0.28 1.7134321928024292 1.8411829471588135 55.696287870407104\n",
      "78 0.0001 0.0001 1.0 0.13 0.28 0.28 1.7095009088516235 1.8396668434143066 56.39010286331177\n",
      "79 0.0001 0.0001 1.0 0.12 0.28 0.28 1.709547996520996 1.835017204284668 57.02959895133972\n",
      "80 0.0001 0.0001 1.0 0.13 0.28 0.28 1.706418514251709 1.8322455883026123 57.664875984191895\n",
      "81 0.0001 0.0001 1.0 0.14 0.31 0.31 1.7033265829086304 1.8291794061660767 58.29905986785889\n",
      "82 0.0001 0.0001 1.0 0.14 0.28 0.28 1.7004667520523071 1.8264639377593994 58.94174003601074\n",
      "83 0.0001 0.0001 1.0 0.14 0.28 0.28 1.696882724761963 1.8231903314590454 59.578166007995605\n",
      "84 0.0001 0.0001 1.0 0.14 0.28 0.28 1.695433259010315 1.8209927082061768 60.23397088050842\n",
      "85 0.0001 0.0001 1.0 0.13 0.28 0.28 1.6974611282348633 1.8185319900512695 60.8803129196167\n",
      "86 0.0001 0.0001 1.0 0.15 0.31 0.31 1.6942180395126343 1.816951036453247 61.53096795082092\n",
      "87 0.0001 0.0001 1.0 0.14 0.28 0.28 1.6903409957885742 1.8130981922149658 62.16241002082825\n",
      "88 0.0001 0.0001 1.0 0.14 0.28 0.28 1.6865774393081665 1.8115496635437012 62.79007601737976\n",
      "89 0.0001 0.0001 1.0 0.14 0.28 0.28 1.6863784790039062 1.8080545663833618 63.41095018386841\n",
      "90 0.0001 0.0001 1.0 0.12 0.28 0.28 1.6848117113113403 1.80716872215271 64.08252501487732\n",
      "91 0.0001 0.0001 1.0 0.14 0.28 0.28 1.6795316934585571 1.803968071937561 64.7296040058136\n",
      "92 0.0001 0.0001 1.0 0.13 0.28 0.28 1.6815375089645386 1.8018101453781128 65.37450790405273\n",
      "93 0.0001 0.0001 1.0 0.14 0.28 0.28 1.6772464513778687 1.8002915382385254 66.06525206565857\n",
      "94 0.0001 0.0001 1.0 0.12 0.28 0.28 1.6745105981826782 1.795920968055725 66.73765897750854\n",
      "95 0.0001 0.0001 1.0 0.14 0.28 0.28 1.6731157302856445 1.7936230897903442 67.3978521823883\n",
      "96 0.0001 0.0001 1.0 0.15 0.31 0.31 1.6699581146240234 1.7921910285949707 68.04553890228271\n",
      "97 0.0001 0.0001 1.0 0.14 0.28 0.28 1.6689525842666626 1.789006233215332 68.70749092102051\n",
      "98 0.0001 0.0001 1.0 0.12 0.28 0.28 1.6673879623413086 1.7867841720581055 69.39559483528137\n",
      "99 0.0001 0.0001 1.0 0.14 0.28 0.28 1.6637529134750366 1.7853862047195435 70.0860869884491\n",
      "\n",
      "=========================================\n",
      "Training distance 5\n",
      "=========================================\n",
      "Total params: 618778\n",
      "epoch, lr, wd, fract. corr. synd, LER_X, LER_Z, LER_tot, test loss, train loss, train time\n",
      "0 0.0001 0.0001 0.23 0.31 0.77 0.77 3.9305546283721924 4.105885028839111 1.411020040512085\n",
      "Model saved to trained_models/d5_DP_best.pth.\n",
      "1 0.0001 0.0001 0.23 0.31 0.77 0.77 2.961041212081909 3.661977767944336 2.5835320949554443\n",
      "2 0.0001 0.0001 0.23 0.31 0.77 0.77 2.536414623260498 2.783111572265625 3.743515968322754\n",
      "3 0.0001 0.0001 0.23 0.31 0.77 0.77 2.3394572734832764 2.504120111465454 4.902524948120117\n",
      "4 0.0001 0.0001 0.23 0.31 0.77 0.77 2.201841354370117 2.373389720916748 6.069753885269165\n",
      "5 0.0001 0.0001 0.23 0.31 0.77 0.77 2.175326108932495 2.28786039352417 7.288711071014404\n",
      "6 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1649467945098877 2.27288556098938 8.449889183044434\n",
      "7 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1606621742248535 2.268465757369995 9.60708212852478\n",
      "8 0.0001 0.0001 0.23 0.31 0.77 0.77 2.157191038131714 2.2665514945983887 10.758363008499146\n",
      "9 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1551616191864014 2.2645466327667236 11.909540891647339\n",
      "10 0.0001 0.0001 0.23 0.31 0.77 0.77 2.152390241622925 2.262932777404785 13.072550058364868\n",
      "11 0.0001 0.0001 0.23 0.31 0.77 0.77 2.150034189224243 2.260932207107544 14.219912052154541\n",
      "12 0.0001 0.0001 0.23 0.31 0.77 0.77 2.149492025375366 2.259139060974121 15.369107961654663\n",
      "13 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1461904048919678 2.2575864791870117 16.549256086349487\n",
      "14 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1480910778045654 2.256505012512207 17.696628093719482\n",
      "15 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1439309120178223 2.254535675048828 18.849763870239258\n",
      "16 0.0001 0.0001 0.23 0.31 0.77 0.77 2.143949508666992 2.253065824508667 19.990336179733276\n",
      "17 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1428892612457275 2.250998020172119 21.133474826812744\n",
      "18 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1395914554595947 2.249825954437256 22.297598123550415\n",
      "19 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1376121044158936 2.248359203338623 23.44520401954651\n",
      "20 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1387596130371094 2.247734308242798 24.613147974014282\n",
      "21 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1368191242218018 2.2452456951141357 25.777985095977783\n",
      "22 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1340172290802 2.2443430423736572 26.927043199539185\n",
      "23 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1346237659454346 2.2437541484832764 28.071495056152344\n",
      "24 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1346962451934814 2.24145245552063 29.23292899131775\n",
      "25 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1336464881896973 2.2410457134246826 30.41858696937561\n",
      "26 0.0001 0.0001 0.23 0.31 0.77 0.77 2.128995180130005 2.2396581172943115 31.565750122070312\n",
      "27 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1318461894989014 2.2377891540527344 32.75507593154907\n",
      "28 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1253764629364014 2.235635280609131 33.95290398597717\n",
      "29 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1279032230377197 2.2351136207580566 35.12696290016174\n",
      "30 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1228010654449463 2.2331736087799072 36.29327392578125\n",
      "31 0.0001 0.0001 0.23 0.31 0.77 0.77 2.125852108001709 2.229928493499756 37.45534801483154\n",
      "32 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1248960494995117 2.2284135818481445 38.61401391029358\n",
      "33 0.0001 0.0001 0.23 0.31 0.77 0.77 2.1143455505371094 2.2250239849090576 39.78787684440613\n",
      "34 0.0001 0.0001 0.23 0.31 0.77 0.77 2.10981822013855 2.218189001083374 40.95897698402405\n",
      "35 0.0001 0.0001 0.23 0.31 0.77 0.77 2.100623846054077 2.206852674484253 42.10481309890747\n",
      "36 0.0001 0.0001 0.23 0.31 0.77 0.77 2.0682225227355957 2.189258098602295 43.24364399909973\n",
      "37 0.0001 0.0001 0.23 0.31 0.77 0.77 2.0522844791412354 2.171969413757324 44.38905096054077\n",
      "38 0.0001 0.0001 0.23 0.31 0.77 0.77 2.035144805908203 2.1509578227996826 45.530760049819946\n",
      "39 0.0001 0.0001 0.23 0.31 0.77 0.77 2.016728639602661 2.1341452598571777 46.67393898963928\n",
      "40 0.0001 0.0001 0.23 0.31 0.77 0.77 2.005932092666626 2.1201171875 47.81412100791931\n",
      "41 0.0001 0.0001 0.23 0.31 0.77 0.77 1.9855142831802368 2.104015588760376 48.957632064819336\n",
      "42 0.0001 0.0001 0.23 0.31 0.77 0.77 1.9743715524673462 2.0892772674560547 50.09890413284302\n",
      "43 0.0001 0.0001 0.23 0.31 0.77 0.77 1.9615591764450073 2.0783863067626953 51.23879408836365\n",
      "44 0.0001 0.0001 0.23 0.31 0.77 0.77 1.9591032266616821 2.067915916442871 52.37932801246643\n",
      "45 0.0001 0.0001 0.23 0.31 0.77 0.77 1.955225944519043 2.0568418502807617 53.51917910575867\n",
      "46 0.0001 0.0001 0.23 0.31 0.77 0.77 1.939649224281311 2.044191360473633 54.66193199157715\n",
      "47 0.0001 0.0001 0.23 0.31 0.77 0.77 1.920201301574707 2.0303397178649902 55.80105996131897\n",
      "48 0.0001 0.0001 0.23 0.31 0.77 0.77 1.906821370124817 2.01906156539917 56.942330837249756\n",
      "49 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8997353315353394 2.0083048343658447 58.08390712738037\n",
      "50 0.0001 0.0001 0.23 0.31 0.77 0.77 1.887253761291504 1.9972422122955322 59.2740318775177\n",
      "51 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8814624547958374 1.9865208864212036 60.45318794250488\n",
      "52 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8713518381118774 1.9770452976226807 61.61391806602478\n",
      "53 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8628824949264526 1.9693940877914429 62.79051089286804\n",
      "54 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8542732000350952 1.9606614112854004 63.95506501197815\n",
      "55 0.0001 0.0001 0.23 0.31 0.77 0.77 1.84905207157135 1.9522053003311157 65.10144591331482\n",
      "56 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8445128202438354 1.945257544517517 66.30676794052124\n",
      "57 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8385165929794312 1.9393287897109985 67.478187084198\n",
      "58 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8356298208236694 1.9326691627502441 68.64464688301086\n",
      "59 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8274554014205933 1.927661418914795 69.79990100860596\n",
      "60 0.0001 0.0001 0.23 0.31 0.77 0.77 1.828417420387268 1.920904517173767 70.9490020275116\n",
      "61 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8205150365829468 1.9190716743469238 72.10997796058655\n",
      "62 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8170146942138672 1.9129903316497803 73.25579285621643\n",
      "63 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8126907348632812 1.908931851387024 74.41512298583984\n",
      "64 0.0001 0.0001 0.23 0.31 0.77 0.77 1.8079725503921509 1.905133843421936 75.56173491477966\n",
      "65 0.0001 0.0001 0.23 0.31 0.77 0.77 1.803995132446289 1.9012025594711304 76.76871395111084\n",
      "66 0.0001 0.0001 0.23 0.31 0.77 0.77 1.7987991571426392 1.897356629371643 77.91207885742188\n",
      "67 0.0001 0.0001 0.23 0.31 0.77 0.77 1.7959957122802734 1.8932254314422607 79.05158710479736\n",
      "68 0.0001 0.0001 0.32 0.31 0.77 0.77 1.797886848449707 1.8897643089294434 80.1913092136383\n",
      "69 0.0001 0.0001 0.32 0.31 0.77 0.77 1.7869104146957397 1.885939359664917 81.33445692062378\n",
      "70 0.0001 0.0001 0.32 0.31 0.77 0.77 1.7823368310928345 1.8803179264068604 82.4754409790039\n",
      "71 0.0001 0.0001 0.32 0.31 0.77 0.77 1.780072808265686 1.8766582012176514 83.61757183074951\n",
      "72 0.0001 0.0001 0.32 0.31 0.77 0.77 1.7812355756759644 1.8740131855010986 84.76566791534424\n",
      "73 0.0001 0.0001 0.32 0.31 0.77 0.77 1.7776142358779907 1.8700132369995117 85.90629410743713\n",
      "74 0.0001 0.0001 0.32 0.31 0.77 0.77 1.7736831903457642 1.8678795099258423 87.04666090011597\n",
      "75 0.0001 0.0001 0.32 0.31 0.77 0.77 1.7688413858413696 1.8646222352981567 88.1873869895935\n",
      "76 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7661434412002563 1.8626899719238281 89.32663202285767\n",
      "77 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7647517919540405 1.8589661121368408 90.46761298179626\n",
      "78 0.0001 0.0001 0.4 0.31 0.77 0.77 1.757950782775879 1.8556619882583618 91.60831117630005\n",
      "79 0.0001 0.0001 0.4 0.31 0.77 0.77 1.755750060081482 1.8523237705230713 92.75258088111877\n",
      "80 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7560081481933594 1.8497042655944824 93.89793586730957\n",
      "81 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7511717081069946 1.8458974361419678 95.04006719589233\n",
      "82 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7510619163513184 1.8445948362350464 96.18115901947021\n",
      "83 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7482026815414429 1.8408493995666504 97.3233060836792\n",
      "84 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7494088411331177 1.8378934860229492 98.46368789672852\n",
      "85 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7431612014770508 1.8360285758972168 99.603924036026\n",
      "86 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7413259744644165 1.8337616920471191 100.76666498184204\n",
      "87 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7379153966903687 1.8324131965637207 101.91349411010742\n",
      "88 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7359051704406738 1.8287465572357178 103.05354809761047\n",
      "89 0.0001 0.0001 0.4 0.31 0.77 0.77 1.733510971069336 1.8261637687683105 104.19672894477844\n",
      "90 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7313858270645142 1.824134111404419 105.34462404251099\n",
      "91 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7287846803665161 1.8217535018920898 106.49050712585449\n",
      "92 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7285566329956055 1.819861650466919 107.65078783035278\n",
      "93 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7255514860153198 1.8187131881713867 108.79561805725098\n",
      "94 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7230719327926636 1.815319299697876 109.93862724304199\n",
      "95 0.0001 0.0001 0.4 0.31 0.77 0.77 1.722880482673645 1.8140909671783447 111.07981705665588\n",
      "96 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7200897932052612 1.8122631311416626 112.2204270362854\n",
      "97 0.0001 0.0001 0.4 0.31 0.77 0.77 1.719539999961853 1.8107532262802124 113.3646788597107\n",
      "98 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7168536186218262 1.8086212873458862 114.50468111038208\n",
      "99 0.0001 0.0001 0.4 0.31 0.77 0.77 1.7142184972763062 1.806740164756775 115.64558291435242\n",
      "\n",
      "=========================================\n",
      "Training distance 7\n",
      "=========================================\n",
      "Total params: 618778\n",
      "epoch, lr, wd, fract. corr. synd, LER_X, LER_Z, LER_tot, test loss, train loss, train time\n",
      "0 0.0001 0.0001 0.05 0.4 0.95 0.95 4.118459224700928 4.392258644104004 4.564649820327759\n",
      "Model saved to trained_models/d7_DP_best.pth.\n",
      "1 0.0001 0.0001 0.05 0.4 0.95 0.95 2.9291889667510986 3.60490345954895 6.803908109664917\n",
      "2 0.0001 0.0001 0.05 0.4 0.95 0.95 2.388383150100708 2.6888551712036133 9.01744294166565\n",
      "3 0.0001 0.0001 0.05 0.4 0.95 0.95 2.3427376747131348 2.522361993789673 11.247874975204468\n",
      "4 0.0001 0.0001 0.05 0.4 0.95 0.95 2.277041435241699 2.436338186264038 13.468722820281982\n",
      "5 0.0001 0.0001 0.05 0.4 0.95 0.95 2.248599052429199 2.4118833541870117 15.683977842330933\n",
      "6 0.0001 0.0001 0.05 0.4 0.95 0.95 2.2327375411987305 2.397277355194092 17.910808086395264\n",
      "7 0.0001 0.0001 0.05 0.4 0.95 0.95 2.2246577739715576 2.389598846435547 20.12584400177002\n",
      "8 0.0001 0.0001 0.05 0.4 0.95 0.95 2.2176735401153564 2.3845040798187256 22.3458731174469\n",
      "9 0.0001 0.0001 0.05 0.4 0.95 0.95 2.212268114089966 2.3801157474517822 24.579596042633057\n",
      "10 0.0001 0.0001 0.05 0.4 0.95 0.95 2.207969903945923 2.3766744136810303 26.81979990005493\n",
      "11 0.0001 0.0001 0.05 0.4 0.95 0.95 2.203867197036743 2.373917579650879 29.063923835754395\n",
      "12 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1998612880706787 2.3711013793945312 31.325347900390625\n",
      "13 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1964380741119385 2.368279218673706 33.57237505912781\n",
      "14 0.0001 0.0001 0.05 0.4 0.95 0.95 2.193258047103882 2.366074562072754 35.81407403945923\n",
      "15 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1934969425201416 2.3638203144073486 38.04639983177185\n",
      "16 0.0001 0.0001 0.05 0.4 0.95 0.95 2.187845468521118 2.361818790435791 40.279178857803345\n",
      "17 0.0001 0.0001 0.05 0.4 0.95 0.95 2.186310052871704 2.3594069480895996 42.4994900226593\n",
      "18 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1840193271636963 2.357247829437256 44.72874593734741\n",
      "19 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1863081455230713 2.3550314903259277 46.95425605773926\n",
      "20 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1794261932373047 2.35361909866333 49.17597007751465\n",
      "21 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1755940914154053 2.3505003452301025 51.39965295791626\n",
      "22 0.0001 0.0001 0.05 0.4 0.95 0.95 2.172322988510132 2.347627639770508 53.63196587562561\n",
      "23 0.0001 0.0001 0.05 0.4 0.95 0.95 2.169754981994629 2.345252513885498 55.86091494560242\n",
      "24 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1702663898468018 2.3426530361175537 58.15893292427063\n",
      "25 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1608381271362305 2.3384170532226562 60.39410877227783\n",
      "26 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1782619953155518 2.3360073566436768 62.60837388038635\n",
      "27 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1586520671844482 2.336562156677246 64.84997296333313\n",
      "28 0.0001 0.0001 0.05 0.4 0.95 0.95 2.144953966140747 2.3262252807617188 67.0784649848938\n",
      "29 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1427204608917236 2.3147497177124023 69.29802012443542\n",
      "30 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1242713928222656 2.3050625324249268 71.53251695632935\n",
      "31 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1522836685180664 2.3036208152770996 73.8016619682312\n",
      "32 0.0001 0.0001 0.05 0.4 0.95 0.95 2.1089868545532227 2.294353723526001 76.07771301269531\n",
      "33 0.0001 0.0001 0.05 0.4 0.95 0.95 2.098581075668335 2.2762861251831055 78.35040402412415\n",
      "34 0.0001 0.0001 0.05 0.4 0.95 0.95 2.0943491458892822 2.2657363414764404 80.58405089378357\n",
      "35 0.0001 0.0001 0.05 0.4 0.95 0.95 2.082693099975586 2.2582497596740723 82.8227858543396\n",
      "36 0.0001 0.0001 0.05 0.4 0.95 0.95 2.0759432315826416 2.249284029006958 85.08750605583191\n",
      "37 0.0001 0.0001 0.05 0.4 0.95 0.95 2.071671485900879 2.2428207397460938 87.33983087539673\n",
      "38 0.0001 0.0001 0.05 0.4 0.95 0.95 2.0606460571289062 2.234999656677246 89.618812084198\n",
      "39 0.0001 0.0001 0.05 0.4 0.95 0.95 2.060032844543457 2.229292631149292 91.91973090171814\n",
      "40 0.0001 0.0001 0.05 0.4 0.95 0.95 2.054788827896118 2.2222445011138916 94.29347801208496\n",
      "41 0.0001 0.0001 0.05 0.4 0.95 0.95 2.045154333114624 2.2182810306549072 96.5873031616211\n",
      "42 0.0001 0.0001 0.05 0.4 0.95 0.95 2.039175271987915 2.213319778442383 98.93571400642395\n",
      "43 0.0001 0.0001 0.05 0.4 0.95 0.95 2.0400171279907227 2.2097511291503906 101.22485089302063\n",
      "44 0.0001 0.0001 0.05 0.4 0.95 0.95 2.0312538146972656 2.204073429107666 103.51958012580872\n",
      "45 0.0001 0.0001 0.05 0.4 0.95 0.95 2.0330264568328857 2.198380708694458 105.81456708908081\n",
      "46 0.0001 0.0001 0.05 0.4 0.95 0.95 2.0327327251434326 2.1983163356781006 108.1011528968811\n",
      "47 0.0001 0.0001 0.05 0.4 0.95 0.95 2.020214319229126 2.1918225288391113 110.3665840625763\n",
      "48 0.0001 0.0001 0.05 0.4 0.95 0.95 2.027575969696045 2.1856558322906494 112.6460189819336\n",
      "49 0.0001 0.0001 0.05 0.4 0.95 0.95 2.0358026027679443 2.181856632232666 114.91239380836487\n",
      "50 0.0001 0.0001 0.05 0.4 0.95 0.95 2.0082666873931885 2.1779189109802246 117.19021797180176\n",
      "51 0.0001 0.0001 0.05 0.4 0.95 0.95 2.0133674144744873 2.168477773666382 119.55117297172546\n",
      "52 0.0001 0.0001 0.05 0.4 0.95 0.95 2.006615400314331 2.1641488075256348 121.83024978637695\n",
      "53 0.0001 0.0001 0.05 0.4 0.95 0.95 2.00066876411438 2.1559715270996094 124.10775184631348\n",
      "54 0.0001 0.0001 0.05 0.4 0.95 0.95 1.9871258735656738 2.152277946472168 126.36073589324951\n",
      "55 0.0001 0.0001 0.05 0.4 0.95 0.95 1.9969624280929565 2.1428260803222656 128.6186089515686\n",
      "56 0.0001 0.0001 0.05 0.4 0.95 0.95 1.9904626607894897 2.144392251968384 130.88177490234375\n",
      "57 0.0001 0.0001 0.05 0.4 0.95 0.95 1.9641284942626953 2.1331629753112793 133.1418538093567\n",
      "58 0.0001 0.0001 0.05 0.4 0.95 0.95 1.955214500427246 2.1168696880340576 135.4071159362793\n",
      "59 0.0001 0.0001 0.05 0.4 0.95 0.95 1.9461442232131958 2.103087902069092 137.66419196128845\n",
      "60 0.0001 0.0001 0.05 0.4 0.95 0.95 1.941200852394104 2.0956501960754395 139.92904901504517\n",
      "61 0.0001 0.0001 0.05 0.4 0.95 0.95 1.9372256994247437 2.089743137359619 142.1916630268097\n",
      "62 0.0001 0.0001 0.05 0.4 0.95 0.95 1.9300845861434937 2.0840935707092285 144.45854783058167\n",
      "63 0.0001 0.0001 0.05 0.4 0.95 0.95 1.927130103111267 2.07944655418396 146.7367069721222\n",
      "64 0.0001 0.0001 0.05 0.4 0.95 0.95 1.9265116453170776 2.07539701461792 149.03577208518982\n",
      "65 0.0001 0.0001 0.05 0.4 0.95 0.95 1.9212878942489624 2.068983316421509 151.34041786193848\n",
      "66 0.0001 0.0001 0.15 0.4 0.95 0.95 1.9124698638916016 2.0638267993927 153.6300299167633\n",
      "67 0.0001 0.0001 0.15 0.4 0.95 0.95 1.9055360555648804 2.0571980476379395 155.9420440196991\n",
      "68 0.0001 0.0001 0.15 0.4 0.95 0.95 1.901205062866211 2.05210018157959 158.24423694610596\n",
      "69 0.0001 0.0001 0.21 0.4 0.95 0.95 1.9042549133300781 2.0498642921447754 160.54558300971985\n",
      "70 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8936043977737427 2.041163206100464 162.83907079696655\n",
      "71 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8953262567520142 2.037382125854492 165.12513399124146\n",
      "72 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8890323638916016 2.0322601795196533 167.42234897613525\n",
      "73 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8845151662826538 2.027238130569458 169.6971399784088\n",
      "74 0.0001 0.0001 0.21 0.4 0.95 0.95 1.88324773311615 2.023503541946411 171.95615100860596\n",
      "75 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8794361352920532 2.0192551612854004 174.21265411376953\n",
      "76 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8748046159744263 2.014794111251831 176.47597694396973\n",
      "77 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8735179901123047 2.014336109161377 178.72704410552979\n",
      "78 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8667703866958618 2.007427215576172 180.99642992019653\n",
      "79 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8630365133285522 2.0013251304626465 183.26528596878052\n",
      "80 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8592500686645508 1.9951541423797607 185.5406928062439\n",
      "81 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8556255102157593 1.9901264905929565 187.8089919090271\n",
      "82 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8509254455566406 1.9877429008483887 190.09548687934875\n",
      "83 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8472837209701538 1.983104944229126 192.3487310409546\n",
      "84 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8431707620620728 1.9791820049285889 194.6150050163269\n",
      "85 0.0001 0.0001 0.21 0.4 0.95 0.95 1.8414250612258911 1.976489543914795 196.87837100028992\n",
      "86 0.0001 0.0001 0.68 0.4 0.95 0.95 1.8419780731201172 1.9727197885513306 199.15340399742126\n",
      "87 0.0001 0.0001 0.68 0.4 0.95 0.95 1.8361454010009766 1.9709627628326416 201.45478796958923\n",
      "88 0.0001 0.0001 0.68 0.4 0.95 0.95 1.8336941003799438 1.9661736488342285 203.7243618965149\n",
      "89 0.0001 0.0001 0.68 0.4 0.95 0.95 1.8299776315689087 1.9630225896835327 205.97747707366943\n",
      "90 0.0001 0.0001 1.0 0.4 0.95 0.95 1.8274282217025757 1.9581573009490967 208.24320602416992\n",
      "91 0.0001 0.0001 1.0 0.4 0.95 0.95 1.8246537446975708 1.9545841217041016 210.52919912338257\n",
      "92 0.0001 0.0001 1.0 0.4 0.95 0.95 1.8253463506698608 1.952341914176941 212.81618285179138\n",
      "93 0.0001 0.0001 1.0 0.4 0.95 0.95 1.8189640045166016 1.9484491348266602 215.09920406341553\n",
      "94 0.0001 0.0001 1.0 0.4 0.95 0.95 1.8184685707092285 1.9451501369476318 217.3821427822113\n",
      "95 0.0001 0.0001 1.0 0.4 0.95 0.95 1.8160871267318726 1.9418848752975464 219.64923691749573\n",
      "96 0.0001 0.0001 1.0 0.4 0.95 0.95 1.8129444122314453 1.9387364387512207 221.922709941864\n",
      "97 0.0001 0.0001 1.0 0.4 0.95 0.95 1.8098701238632202 1.935097336769104 224.2242510318756\n",
      "98 0.0001 0.0001 1.0 0.4 0.95 0.95 1.8065199851989746 1.9321099519729614 226.5023238658905\n",
      "99 0.0001 0.0001 1.0 0.4 0.95 0.95 1.8033090829849243 1.9286879301071167 228.807954788208\n",
      "\n",
      "=========================================\n",
      "Training distance 9\n",
      "=========================================\n",
      "Total params: 618778\n",
      "epoch, lr, wd, fract. corr. synd, LER_X, LER_Z, LER_tot, test loss, train loss, train time\n",
      "0 0.0001 0.0001 0.02 0.33 0.98 0.98 3.3882992267608643 3.629810333251953 12.8598153591156\n",
      "Model saved to trained_models/d9_DP_best.pth.\n",
      "1 0.0001 0.0001 0.02 0.33 0.98 0.98 2.9110682010650635 3.308955430984497 18.26532816886902\n",
      "2 0.0001 0.0001 0.02 0.33 0.98 0.98 2.3494293689727783 2.765749216079712 22.884409189224243\n",
      "3 0.0001 0.0001 0.02 0.33 0.98 0.98 2.2196524143218994 2.4351789951324463 27.17037796974182\n",
      "4 0.0001 0.0001 0.02 0.33 0.98 0.98 2.21423602104187 2.397091865539551 31.45679211616516\n",
      "5 0.0001 0.0001 0.02 0.33 0.98 0.98 2.209054708480835 2.3902432918548584 35.71926236152649\n",
      "6 0.0001 0.0001 0.02 0.33 0.98 0.98 2.2052347660064697 2.386126756668091 39.962669134140015\n",
      "7 0.0001 0.0001 0.02 0.33 0.98 0.98 2.2007551193237305 2.3824872970581055 44.20574116706848\n",
      "8 0.0001 0.0001 0.02 0.33 0.98 0.98 2.1984386444091797 2.3787286281585693 48.44761919975281\n",
      "9 0.0001 0.0001 0.02 0.33 0.98 0.98 2.1952810287475586 2.3764827251434326 52.6944100856781\n",
      "10 0.0001 0.0001 0.02 0.33 0.98 0.98 2.191903591156006 2.373008966445923 56.94398808479309\n",
      "11 0.0001 0.0001 0.02 0.33 0.98 0.98 2.186480760574341 2.36915922164917 61.39047622680664\n",
      "12 0.0001 0.0001 0.02 0.33 0.98 0.98 2.1826488971710205 2.3635973930358887 65.65307331085205\n",
      "13 0.0001 0.0001 0.02 0.33 0.98 0.98 2.143160820007324 2.347186326980591 69.90036034584045\n",
      "14 0.0001 0.0001 0.02 0.33 0.98 0.98 2.0905838012695312 2.3092236518859863 74.1462721824646\n",
      "15 0.0001 0.0001 0.02 0.33 0.98 0.98 2.1163201332092285 2.3059425354003906 78.38879823684692\n",
      "16 0.0001 0.0001 0.02 0.33 0.98 0.98 2.0601701736450195 2.285372257232666 82.65669417381287\n",
      "17 0.0001 0.0001 0.02 0.33 0.98 0.98 2.045938491821289 2.254171848297119 86.92211723327637\n",
      "18 0.0001 0.0001 0.02 0.33 0.98 0.98 2.03719162940979 2.2422003746032715 91.4459331035614\n",
      "19 0.0001 0.0001 0.02 0.33 0.98 0.98 2.0286381244659424 2.2327070236206055 95.70327115058899\n",
      "20 0.0001 0.0001 0.02 0.33 0.98 0.98 2.01997971534729 2.223318338394165 99.99076199531555\n",
      "21 0.0001 0.0001 0.02 0.33 0.98 0.98 2.0102808475494385 2.214691638946533 104.23802924156189\n",
      "22 0.0001 0.0001 0.02 0.33 0.98 0.98 2.0096518993377686 2.2056751251220703 108.48204708099365\n",
      "23 0.0001 0.0001 0.02 0.33 0.98 0.98 2.0064423084259033 2.19948148727417 112.73351907730103\n",
      "24 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9957932233810425 2.190993070602417 116.96641516685486\n",
      "25 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9939817190170288 2.1832222938537598 121.18842124938965\n",
      "26 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9868059158325195 2.1935458183288574 125.42361426353455\n",
      "27 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9776438474655151 2.169715642929077 129.66953706741333\n",
      "28 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9698957204818726 2.1585350036621094 133.98563408851624\n",
      "29 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9560493230819702 2.151089668273926 138.5695023536682\n",
      "30 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9940528869628906 2.1569273471832275 142.7974090576172\n",
      "31 0.0001 0.0001 0.02 0.33 0.98 0.98 1.962346076965332 2.139636754989624 147.03028512001038\n",
      "32 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9641566276550293 2.1253299713134766 151.27466702461243\n",
      "33 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9545038938522339 2.1150264739990234 155.48459696769714\n",
      "34 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9413508176803589 2.106649160385132 159.7096381187439\n",
      "35 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9453157186508179 2.099717140197754 163.94234538078308\n",
      "36 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9153920412063599 2.094259738922119 168.16558814048767\n",
      "37 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9222736358642578 2.083200216293335 172.38404726982117\n",
      "38 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9206651449203491 2.0782294273376465 176.6011950969696\n",
      "39 0.0001 0.0001 0.02 0.33 0.98 0.98 1.9025774002075195 2.071113348007202 180.8681402206421\n",
      "40 0.0001 0.0001 0.02 0.33 0.98 0.98 1.893001914024353 2.062943458557129 185.45453810691833\n",
      "41 0.0001 0.0001 0.02 0.33 0.98 0.98 1.8982700109481812 2.056610107421875 190.17051315307617\n",
      "42 0.0001 0.0001 0.02 0.33 0.98 0.98 1.8923386335372925 2.0522825717926025 194.8027310371399\n",
      "43 0.0001 0.0001 0.08 0.33 0.98 0.98 1.8793214559555054 2.0441527366638184 199.4046220779419\n",
      "44 0.0001 0.0001 0.08 0.33 0.98 0.98 1.8800455331802368 2.0373995304107666 203.69673705101013\n",
      "45 0.0001 0.0001 0.08 0.33 0.98 0.98 1.8730779886245728 2.032644748687744 207.95868015289307\n",
      "46 0.0001 0.0001 0.08 0.33 0.98 0.98 1.8629026412963867 2.0270326137542725 212.2279303073883\n",
      "47 0.0001 0.0001 0.11 0.33 0.98 0.98 1.8618282079696655 2.021233320236206 216.45018315315247\n",
      "48 0.0001 0.0001 0.08 0.33 0.98 0.98 1.8556350469589233 2.01226806640625 220.6805031299591\n",
      "49 0.0001 0.0001 0.08 0.33 0.98 0.98 1.8467398881912231 2.0042872428894043 224.90353202819824\n",
      "50 0.0001 0.0001 0.31 0.33 0.98 0.98 1.8420089483261108 1.999948263168335 229.15986514091492\n",
      "51 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8390212059020996 1.995690941810608 233.44209718704224\n",
      "52 0.0001 0.0001 1.0 0.33 0.98 0.98 1.837456226348877 1.992061972618103 237.6699299812317\n",
      "53 0.0001 0.0001 1.0 0.33 0.98 0.98 1.833630919456482 1.9884026050567627 242.27194905281067\n",
      "54 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8296221494674683 1.9847540855407715 246.52360606193542\n",
      "55 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8257070779800415 1.9816882610321045 250.7458622455597\n",
      "56 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8245896100997925 1.9787228107452393 254.9887022972107\n",
      "57 0.0001 0.0001 1.0 0.33 0.98 0.98 1.820874810218811 1.9757870435714722 259.19810819625854\n",
      "58 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8185911178588867 1.9713765382766724 263.4465732574463\n",
      "59 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8156824111938477 1.9682142734527588 267.6857280731201\n",
      "60 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8151793479919434 1.9655799865722656 271.9426863193512\n",
      "61 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8105114698410034 1.9628148078918457 276.2019262313843\n",
      "62 0.0001 0.0001 1.0 0.33 0.98 0.98 1.808831810951233 1.9588935375213623 280.4440071582794\n",
      "63 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8052841424942017 1.9550940990447998 284.66794323921204\n",
      "64 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8031868934631348 1.9514782428741455 288.8974521160126\n",
      "65 0.0001 0.0001 1.0 0.33 0.98 0.98 1.8005619049072266 1.9476059675216675 293.1223442554474\n",
      "66 0.0001 0.0001 1.0 0.33 0.98 0.98 1.796958565711975 1.943565845489502 297.35461616516113\n",
      "67 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7949937582015991 1.9400384426116943 301.61296916007996\n",
      "68 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7914366722106934 1.9364922046661377 305.88137316703796\n",
      "69 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7894182205200195 1.9325964450836182 310.1129159927368\n",
      "70 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7878142595291138 1.929599642753601 314.36908626556396\n",
      "71 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7845362424850464 1.9260505437850952 318.59887313842773\n",
      "72 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7814291715621948 1.9219446182250977 322.83092403411865\n",
      "73 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7790952920913696 1.918473243713379 327.08413529396057\n",
      "74 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7769780158996582 1.9153807163238525 331.36112117767334\n",
      "75 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7754119634628296 1.9130443334579468 335.58135414123535\n",
      "76 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7734193801879883 1.9102474451065063 339.80542731285095\n",
      "77 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7705146074295044 1.9067909717559814 344.053346157074\n",
      "78 0.0001 0.0001 1.0 0.33 0.98 0.98 1.768150806427002 1.9042421579360962 348.3621451854706\n",
      "79 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7657438516616821 1.9015371799468994 352.6642792224884\n",
      "80 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7624038457870483 1.8972957134246826 356.9141821861267\n",
      "81 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7600765228271484 1.8931505680084229 361.1763153076172\n",
      "82 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7565444707870483 1.8889846801757812 365.4220471382141\n",
      "83 0.0001 0.0001 1.0 0.33 0.98 0.98 1.753179907798767 1.8853293657302856 369.9537992477417\n",
      "84 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7503496408462524 1.8808075189590454 374.1930522918701\n",
      "85 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7476760149002075 1.8775599002838135 378.4233453273773\n",
      "86 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7450919151306152 1.8742895126342773 382.66656827926636\n",
      "87 0.0001 0.0001 1.0 0.33 0.98 0.98 1.742916226387024 1.8713269233703613 386.8851079940796\n",
      "88 0.0001 0.0001 1.0 0.33 0.98 0.98 1.739737868309021 1.8680559396743774 391.10776233673096\n",
      "89 0.0001 0.0001 1.0 0.33 0.98 0.98 1.735856533050537 1.8642293214797974 395.3770751953125\n",
      "90 0.0001 0.0001 1.0 0.33 0.98 0.98 1.733030915260315 1.8582781553268433 399.6275041103363\n",
      "91 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7303608655929565 1.854519009590149 403.8772931098938\n",
      "92 0.0001 0.0001 1.0 0.33 0.98 0.98 1.72732675075531 1.8524017333984375 408.5242340564728\n",
      "93 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7246407270431519 1.8480191230773926 413.0476050376892\n",
      "94 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7231764793395996 1.845271110534668 417.3069350719452\n",
      "95 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7202130556106567 1.842393398284912 421.5587692260742\n",
      "96 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7183202505111694 1.8401167392730713 425.80401730537415\n",
      "97 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7193254232406616 1.8379790782928467 430.0455582141876\n",
      "98 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7152570486068726 1.8352330923080444 434.291038274765\n",
      "99 0.0001 0.0001 1.0 0.33 0.98 0.98 1.7121896743774414 1.8327175378799438 438.57145619392395\n",
      "\n",
      "=========================================\n",
      "Training distance 11\n",
      "=========================================\n",
      "Total params: 618778\n",
      "epoch, lr, wd, fract. corr. synd, LER_X, LER_Z, LER_tot, test loss, train loss, train time\n",
      "0 0.0001 0.0001 0.0 0.39 1.0 1.0 3.728961229324341 3.850351572036743 14.923181056976318\n",
      "Model saved to trained_models/d11_DP_best.pth.\n",
      "1 0.0001 0.0001 0.0 0.39 1.0 1.0 3.1848440170288086 3.5903642177581787 22.439656019210815\n",
      "2 0.0001 0.0001 0.0 0.39 1.0 1.0 2.538222074508667 2.941742181777954 30.086580991744995\n",
      "3 0.0001 0.0001 0.0 0.39 1.0 1.0 2.2961020469665527 2.5373547077178955 37.47096300125122\n",
      "4 0.0001 0.0001 0.0 0.39 1.0 1.0 2.146005868911743 2.4205331802368164 43.95218300819397\n",
      "5 0.0001 0.0001 0.0 0.39 1.0 1.0 2.124807357788086 2.3178420066833496 51.255715131759644\n",
      "6 0.0001 0.0001 0.0 0.39 1.0 1.0 2.1066510677337646 2.302155017852783 58.600656032562256\n",
      "7 0.0001 0.0001 0.0 0.39 1.0 1.0 2.1030735969543457 2.2956361770629883 65.59532809257507\n",
      "8 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0993049144744873 2.292217493057251 72.93002796173096\n",
      "9 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0947697162628174 2.2881882190704346 79.40646409988403\n",
      "10 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0918710231781006 2.285029172897339 86.65994501113892\n",
      "11 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0893476009368896 2.283447265625 93.9579131603241\n",
      "12 0.0001 0.0001 0.0 0.39 1.0 1.0 2.087531328201294 2.2817163467407227 101.30914998054504\n",
      "13 0.0001 0.0001 0.0 0.39 1.0 1.0 2.085522413253784 2.2805612087249756 107.79243278503418\n",
      "14 0.0001 0.0001 0.0 0.39 1.0 1.0 2.084286689758301 2.2792932987213135 114.17640805244446\n",
      "15 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0838096141815186 2.2783944606781006 121.45826292037964\n",
      "16 0.0001 0.0001 0.0 0.39 1.0 1.0 2.083425283432007 2.2774951457977295 129.21662306785583\n",
      "17 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0822832584381104 2.276535749435425 136.64126586914062\n",
      "18 0.0001 0.0001 0.0 0.39 1.0 1.0 2.08298921585083 2.2756407260894775 143.66362810134888\n",
      "19 0.0001 0.0001 0.0 0.39 1.0 1.0 2.079697847366333 2.274479389190674 150.66375303268433\n",
      "20 0.0001 0.0001 0.0 0.39 1.0 1.0 2.07812237739563 2.272573709487915 158.0511040687561\n",
      "21 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0934975147247314 2.2723546028137207 164.8302059173584\n",
      "22 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0876474380493164 2.2737069129943848 172.1117560863495\n",
      "23 0.0001 0.0001 0.0 0.39 1.0 1.0 2.075181722640991 2.2695295810699463 178.65031814575195\n",
      "24 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0715410709381104 2.265341281890869 185.9342279434204\n",
      "25 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0721466541290283 2.263927698135376 193.70045804977417\n",
      "26 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0703628063201904 2.2611212730407715 200.11543083190918\n",
      "27 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0759735107421875 2.2667160034179688 206.54076099395752\n",
      "28 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0669870376586914 2.259805679321289 212.936176776886\n",
      "29 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0614349842071533 2.2542760372161865 219.67241597175598\n",
      "30 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0629382133483887 2.2508273124694824 226.90536403656006\n",
      "31 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0447795391082764 2.2470719814300537 233.9947919845581\n",
      "32 0.0001 0.0001 0.0 0.39 1.0 1.0 2.037304639816284 2.23945951461792 241.08954691886902\n",
      "33 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0276870727539062 2.2304835319519043 248.56905794143677\n",
      "34 0.0001 0.0001 0.0 0.39 1.0 1.0 2.1141154766082764 2.2197492122650146 256.3976790904999\n",
      "35 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0910556316375732 2.2978382110595703 263.74299693107605\n",
      "36 0.0001 0.0001 0.0 0.39 1.0 1.0 2.037721872329712 2.2404098510742188 271.11361622810364\n",
      "37 0.0001 0.0001 0.0 0.39 1.0 1.0 2.018691062927246 2.2062830924987793 277.85670804977417\n",
      "38 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0057740211486816 2.1870951652526855 285.0384051799774\n",
      "39 0.0001 0.0001 0.0 0.39 1.0 1.0 2.0194246768951416 2.1731839179992676 291.4415228366852\n",
      "40 0.0001 0.0001 0.0 0.39 1.0 1.0 1.974377989768982 2.172860860824585 297.8090901374817\n",
      "41 0.0001 0.0001 0.0 0.39 1.0 1.0 1.9500240087509155 2.1336493492126465 304.18856406211853\n",
      "42 0.0001 0.0001 0.0 0.39 1.0 1.0 1.9509421586990356 2.1068532466888428 310.53728699684143\n",
      "43 0.0001 0.0001 0.0 0.39 1.0 1.0 1.9046611785888672 2.0830276012420654 317.73976707458496\n",
      "44 0.0001 0.0001 0.01 0.39 1.0 1.0 1.8884774446487427 2.063164234161377 324.6188690662384\n",
      "45 0.0001 0.0001 0.02 0.33 1.0 1.0 1.8630918264389038 2.032569646835327 331.0489511489868\n",
      "46 0.0001 0.0001 0.02 0.33 1.0 1.0 1.8194812536239624 1.9953064918518066 338.3663749694824\n",
      "47 0.0001 0.0001 0.01 0.33 1.0 1.0 1.7370208501815796 1.9518085718154907 345.38588309288025\n",
      "48 0.0001 0.0001 0.1 0.37 1.0 1.0 1.6189900636672974 1.8620463609695435 352.1712679862976\n",
      "49 0.0001 0.0001 0.73 0.39 1.0 1.0 1.5144344568252563 1.7663418054580688 359.4932110309601\n",
      "50 0.0001 0.0001 0.59 0.39 1.0 1.0 1.4633830785751343 1.6892989873886108 367.00585198402405\n",
      "51 0.0001 0.0001 1.0 0.39 1.0 1.0 1.4049001932144165 1.6432151794433594 374.79995012283325\n",
      "52 0.0001 0.0001 0.92 0.39 1.0 1.0 1.3764203786849976 1.6083834171295166 382.301176071167\n",
      "53 0.0001 0.0001 0.94 0.39 1.0 1.0 1.3537801504135132 1.587178111076355 390.1724548339844\n",
      "54 0.0001 0.0001 1.0 0.33 1.0 1.0 1.3320529460906982 1.560023546218872 397.6388158798218\n",
      "55 0.0001 0.0001 1.0 0.33 0.99 0.99 1.3091374635696411 1.5378150939941406 405.80638098716736\n",
      "Model saved to trained_models/d11_DP_best.pth.\n",
      "56 0.0001 0.0001 1.0 0.33 0.99 0.99 1.286411166191101 1.5132417678833008 413.5927140712738\n",
      "57 0.0001 0.0001 1.0 0.33 0.99 0.99 1.279455304145813 1.4991655349731445 420.95941615104675\n",
      "58 0.0001 0.0001 0.99 0.33 0.99 0.99 1.2580641508102417 1.4860340356826782 428.8629500865936\n",
      "59 0.0001 0.0001 0.99 0.33 0.99 0.99 1.2422384023666382 1.4721359014511108 436.5818691253662\n",
      "60 0.0001 0.0001 0.99 0.33 0.99 0.99 1.2340574264526367 1.4626034498214722 444.7960398197174\n",
      "61 0.0001 0.0001 0.99 0.33 0.99 0.99 1.224082350730896 1.4528894424438477 451.70161294937134\n",
      "62 0.0001 0.0001 0.94 0.33 0.99 0.99 1.216042399406433 1.4453811645507812 458.12870812416077\n",
      "63 0.0001 0.0001 0.94 0.33 0.99 0.99 1.2087186574935913 1.4379637241363525 464.8412039279938\n",
      "64 0.0001 0.0001 0.99 0.33 0.99 0.99 1.203858733177185 1.4315261840820312 471.387659072876\n",
      "65 0.0001 0.0001 0.94 0.33 0.99 0.99 1.1951748132705688 1.4243439435958862 477.8396260738373\n",
      "66 0.0001 0.0001 0.94 0.33 0.99 0.99 1.1901201009750366 1.4182326793670654 485.1957759857178\n",
      "67 0.0001 0.0001 0.99 0.33 0.99 0.99 1.1796901226043701 1.4108924865722656 492.92410588264465\n",
      "68 0.0001 0.0001 0.99 0.33 0.99 0.99 1.176179051399231 1.4036118984222412 500.0877981185913\n",
      "69 0.0001 0.0001 1.0 0.33 0.99 0.99 1.166699767112732 1.397693157196045 506.73029494285583\n",
      "70 0.0001 0.0001 1.0 0.33 0.99 0.99 1.1612266302108765 1.3902640342712402 513.4358389377594\n",
      "71 0.0001 0.0001 1.0 0.33 0.99 0.99 1.1539292335510254 1.3855111598968506 520.2144980430603\n",
      "72 0.0001 0.0001 1.0 0.33 0.99 0.99 1.1450241804122925 1.3788535594940186 526.8671958446503\n",
      "73 0.0001 0.0001 1.0 0.33 0.99 0.99 1.1385350227355957 1.3714122772216797 533.509407043457\n",
      "74 0.0001 0.0001 1.0 0.33 0.99 0.99 1.1316784620285034 1.3661282062530518 540.3540079593658\n",
      "75 0.0001 0.0001 1.0 0.33 0.99 0.99 1.1252927780151367 1.3602991104125977 547.4670779705048\n",
      "76 0.0001 0.0001 1.0 0.33 0.99 0.99 1.1207486391067505 1.3545793294906616 553.8745400905609\n",
      "77 0.0001 0.0001 1.0 0.33 0.99 0.99 1.1139882802963257 1.3492361307144165 560.2170219421387\n",
      "78 0.0001 0.0001 1.0 0.33 0.99 0.99 1.107177734375 1.3434427976608276 566.591429233551\n",
      "79 0.0001 0.0001 1.0 0.33 0.99 0.99 1.1025232076644897 1.338606595993042 572.9747309684753\n",
      "80 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0989292860031128 1.3344535827636719 579.4049069881439\n",
      "81 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0939570665359497 1.3295496702194214 585.7654168605804\n",
      "82 0.0001 0.0001 1.0 0.33 0.99 0.99 1.089542269706726 1.3253941535949707 592.1127018928528\n",
      "83 0.0001 0.0001 1.0 0.33 0.99 0.99 1.085591197013855 1.3217273950576782 598.4641060829163\n",
      "84 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0751537084579468 1.3169386386871338 605.6956219673157\n",
      "85 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0700411796569824 1.3097246885299683 612.1099379062653\n",
      "86 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0660475492477417 1.305478572845459 618.4604029655457\n",
      "87 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0620983839035034 1.3022410869598389 625.3966147899628\n",
      "88 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0591238737106323 1.2985973358154297 632.6543049812317\n",
      "89 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0568115711212158 1.2955306768417358 639.2165491580963\n",
      "90 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0538902282714844 1.292711615562439 646.5987010002136\n",
      "91 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0507298707962036 1.2896777391433716 653.102126121521\n",
      "92 0.0001 0.0001 1.0 0.33 0.99 0.99 1.047608494758606 1.285604476928711 659.9005959033966\n",
      "93 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0450776815414429 1.2832850217819214 667.2276909351349\n",
      "94 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0420995950698853 1.2807443141937256 674.8203730583191\n",
      "95 0.0001 0.0001 1.0 0.33 0.99 0.99 1.040382981300354 1.2787246704101562 683.020800113678\n",
      "96 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0380958318710327 1.2761951684951782 690.4362502098083\n",
      "97 0.0001 0.0001 1.0 0.33 0.99 0.99 1.032214641571045 1.2728004455566406 697.2838399410248\n",
      "98 0.0001 0.0001 1.0 0.33 0.99 0.99 1.0287762880325317 1.2675639390945435 704.1004469394684\n",
      "99 0.0001 0.0001 1.0 0.33 0.99 0.99 1.025769591331482 1.2647409439086914 710.5135900974274\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# main loop over distances\n",
    "# ======================================================\n",
    "for d in distances:\n",
    "    dist = d\n",
    "    print(\"\\n=========================================\")\n",
    "    print(\"Training distance\", d)\n",
    "    print(\"=========================================\")\n",
    "    best_model_path = f\"trained_models/d{dist}_{error_model_name}_best.pth\"\n",
    "    best_ler_tot = float(\"inf\")\n",
    "\n",
    "    # ---------------- create code & GNN -------------\n",
    "    code = surface_2d.RotatedPlanar2DCode(dist)\n",
    "    gnn = GNNDecoder(\n",
    "        dist=dist,\n",
    "        n_node_inputs=n_node_inputs,\n",
    "        n_node_outputs=n_node_outputs,\n",
    "        n_iters=n_iters,\n",
    "        n_node_features=n_node_features,\n",
    "        n_edge_features=n_edge_features,\n",
    "        msg_net_size=msg_net_size,\n",
    "        msg_net_dropout_p=msg_net_dropout_p,\n",
    "        gru_dropout_p=gru_dropout_p\n",
    "    ).to(device)\n",
    "\n",
    "    src, tgt = surface_code_edges(code)\n",
    "    src_tensor = torch.LongTensor(src)\n",
    "    tgt_tensor = torch.LongTensor(tgt)\n",
    "    GNNDecoder.surface_code_edges = (src_tensor, tgt_tensor)\n",
    "\n",
    "    Hx_dense = np.asarray(code.Hx.toarray(), dtype=np.uint8)\n",
    "    Hz_dense = np.asarray(code.Hz.toarray(), dtype=np.uint8)\n",
    "\n",
    "    hx_null = nullspace(Hx_dense)\n",
    "    hz_null = nullspace(Hz_dense)\n",
    "\n",
    "# nullspace() returns a scipy sparse matrix â†’ make it dense\n",
    "    if hasattr(hx_null, \"toarray\"):\n",
    "        hx_null = hx_null.toarray()\n",
    "    if hasattr(hz_null, \"toarray\"):\n",
    "        hz_null = hz_null.toarray()\n",
    "\n",
    "    hx_null = np.asarray(hx_null, dtype=np.float32)\n",
    "    hz_null = np.asarray(hz_null, dtype=np.float32)\n",
    "\n",
    "    hxperp = torch.tensor(hx_null, dtype=torch.float32, device=device)\n",
    "    hzperp = torch.tensor(hz_null, dtype=torch.float32, device=device)\n",
    "    GNNDecoder.hxperp = hxperp\n",
    "    GNNDecoder.hzperp = hzperp\n",
    "    GNNDecoder.device = device\n",
    "\n",
    "    total_params = sum(p.numel() for p in gnn.parameters())\n",
    "    print(\"Total params:\", total_params)\n",
    "\n",
    "    # ---------- optional warm-start loading ----------\n",
    "    fnameload = (\n",
    "        f\"trained_models/d{d}_{error_model_name}_30_500_500_200000_0.15_10000_0.05_gnn.pth 0.0144_0.0044 37\"\n",
    "    )\n",
    "    model_loaded = False\n",
    "    if os.path.isfile(fnameload):\n",
    "        load_model(gnn, fnameload, device)\n",
    "        model_loaded = True\n",
    "        print(\"Loaded pre-trained model:\", fnameload)\n",
    "\n",
    "    # ---------- test set (monitoring) ---------------\n",
    "    testset = adapt_trainset(\n",
    "        generate_syndrome_error_volume(\n",
    "            code, error_model=error_model,\n",
    "            p=test_err_rate,\n",
    "            batch_size=len_test_set,\n",
    "            for_training=False\n",
    "        ),\n",
    "        code, num_classes=n_node_inputs, for_training=False\n",
    "    )\n",
    "    testloader = DataLoader(testset, batch_size=512,\n",
    "                            collate_fn=collate, shuffle=False)\n",
    "\n",
    "    # ---------- output filename base -----------------\n",
    "    fnamenew = (\n",
    "        f\"trained_models/d{dist}_{error_model_name}_\"\n",
    "        f\"{n_iters}_{n_node_features}_{n_edge_features}_\"\n",
    "        f\"{len_train_set}_{max_train_err_rate}_{len_test_set}_\"\n",
    "        f\"{test_err_rate}_{lr}_{weight_decay}_\"\n",
    "        f\"{msg_net_size}_{msg_net_dropout_p}_{gru_dropout_p}_\"\n",
    "    )\n",
    "    if model_loaded:\n",
    "        fnamenew = (\n",
    "            f\"trained_models/d{dist}_from_d{d}_{error_model_name}_\"\n",
    "            f\"{n_iters}_{n_node_features}_{n_edge_features}_\"\n",
    "            f\"{len_train_set}_{max_train_err_rate}_{len_test_set}_\"\n",
    "            f\"{test_err_rate}_{lr}_{weight_decay}_\"\n",
    "            f\"{msg_net_size}_{msg_net_dropout_p}_{gru_dropout_p}_\"\n",
    "        )\n",
    "\n",
    "    # ---------- optimizer & scheduler ----------------\n",
    "    optimizer = optim.AdamW(gnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    exploration_samples = 10 ** 7\n",
    "    lr_reduce_epoch_step = exploration_samples // len_train_set\n",
    "    max_training_data = 10 ** 8\n",
    "    end_training_epoch = max_training_data // len_train_set\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=lr_reduce_epoch_step, gamma=0.1\n",
    "    )\n",
    "\n",
    "    scaler = None\n",
    "\n",
    "    le_rates = np.zeros((epochs, 5), dtype=float)\n",
    "    start_time = time.time()\n",
    "\n",
    "    size = 2 * GNNDecoder.dist ** 2 - 1\n",
    "    error_index = GNNDecoder.dist ** 2 - 1\n",
    "\n",
    "    min_test_err_rate = test_err_rate\n",
    "    min_lerz = test_err_rate\n",
    "\n",
    "    # ---------- training data (generated once) -------\n",
    "    trainset = adapt_trainset(\n",
    "        generate_syndrome_error_volume(\n",
    "            code, error_model, p=max_train_err_rate,\n",
    "            batch_size=len_train_set\n",
    "        ),\n",
    "        code, num_classes=n_node_inputs\n",
    "    )\n",
    "    trainloader = DataLoader(\n",
    "        trainset, batch_size=batch_size, collate_fn=collate, shuffle=False\n",
    "    )\n",
    "\n",
    "    print(\"epoch, lr, wd, fract. corr. synd, LER_X, LER_Z, LER_tot, \"\n",
    "          \"test loss, train loss, train time\")\n",
    "\n",
    "    # =================================================\n",
    "    # epoch loop\n",
    "    # =================================================\n",
    "    for epoch in range(epochs):\n",
    "        gnn.train()\n",
    "        if epoch == end_training_epoch:\n",
    "            break\n",
    "\n",
    "        epoch_loss = []\n",
    "        for i, (inputs, targets, src_ids, dst_ids) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            src_ids, dst_ids = src_ids.to(device), dst_ids.to(device)\n",
    "            loss = 0.0\n",
    "\n",
    "            outputs = gnn(inputs, src_ids, dst_ids)\n",
    "            for out in outputs:\n",
    "                out_view = out.view(-1, size, n_node_inputs)\n",
    "                targ_view = targets.view(-1, size)\n",
    "\n",
    "                eloss = criterion(\n",
    "                    out_view[:, error_index:].reshape(-1, n_node_inputs),\n",
    "                    targ_view[:, error_index:].flatten()\n",
    "                )\n",
    "                sloss = criterion(\n",
    "                    out_view[:, :error_index].reshape(-1, n_node_inputs),\n",
    "                    targ_view[:, :error_index].flatten()\n",
    "                )\n",
    "\n",
    "                loss = loss + ler_loss(out, targets, code) + sloss + eloss\n",
    "\n",
    "            loss = loss / outputs.shape[0]\n",
    "\n",
    "            if scaler is not None:      # CUDA AMP\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:                       # MPS / CPU\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss.append(loss.detach())\n",
    "\n",
    "        epoch_loss = torch.mean(torch.tensor(epoch_loss)).item()\n",
    "\n",
    "       \n",
    "        fraction_solved = fraction_of_solved_puzzles(gnn, testloader, code)\n",
    "        test_loss = compute_accuracy(gnn, testloader, code)\n",
    "        lerx, lerz, ler_tot = logical_error_rate(gnn, testloader, code)\n",
    "\n",
    "        scheduler.step()\n",
    "        le_rates[epoch, 0] = lerx\n",
    "        le_rates[epoch, 1] = lerz\n",
    "        le_rates[epoch, 2] = ler_tot\n",
    "        le_rates[epoch, 3] = test_loss\n",
    "        le_rates[epoch, 4] = epoch_loss\n",
    "        curr_time = time.time() - start_time\n",
    "\n",
    "        print(epoch, optimizer.param_groups[0]['lr'],\n",
    "              optimizer.param_groups[0][\"weight_decay\"],\n",
    "              fraction_solved, lerx, lerz, ler_tot,\n",
    "              test_loss, epoch_loss, curr_time)\n",
    "\n",
    "        # ---- save best model by total LER only ----\n",
    "        if ler_tot < best_ler_tot:\n",
    "            best_ler_tot = ler_tot\n",
    "            save_model(gnn, best_model_path, confirm=False)\n",
    "\n",
    "        # ---- (optional) save training history every 10 epochs ----\n",
    "        if epoch % 10 == 0:\n",
    "            np.save(fnamenew + 'training_lers_and_losses.npy', le_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b5e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================\n",
      "Training distance 3\n",
      "=========================================\n",
      "[d=3] epoch 1, loss=17.144\n",
      "[d=3] epoch 2, loss=10.583\n",
      "[d=3] epoch 3, loss=8.167\n",
      "[d=3] epoch 4, loss=7.454\n",
      "[d=3] epoch 5, loss=7.086\n",
      "[d=3] epoch 6, loss=6.762\n",
      "[d=3] epoch 7, loss=6.262\n",
      "[d=3] epoch 8, loss=6.141\n",
      "[d=3] epoch 9, loss=5.920\n",
      "[d=3] epoch 10, loss=5.753\n",
      "[d=3] epoch 11, loss=5.565\n",
      "[d=3] epoch 12, loss=5.401\n",
      "[d=3] epoch 13, loss=5.274\n",
      "[d=3] epoch 14, loss=5.141\n",
      "[d=3] epoch 15, loss=5.088\n",
      "[d=3] epoch 16, loss=4.957\n",
      "[d=3] epoch 17, loss=4.828\n",
      "[d=3] epoch 18, loss=4.771\n",
      "[d=3] epoch 19, loss=4.734\n",
      "[d=3] epoch 20, loss=4.587\n",
      "[d=3] epoch 21, loss=4.620\n",
      "[d=3] epoch 22, loss=4.477\n",
      "[d=3] epoch 23, loss=4.492\n",
      "[d=3] epoch 24, loss=4.460\n",
      "[d=3] epoch 25, loss=4.458\n",
      "[d=3] epoch 26, loss=4.361\n",
      "[d=3] epoch 27, loss=4.319\n",
      "[d=3] epoch 28, loss=4.276\n",
      "[d=3] epoch 29, loss=4.341\n",
      "[d=3] epoch 30, loss=4.223\n",
      "[d=3] epoch 31, loss=4.190\n",
      "[d=3] epoch 32, loss=4.171\n",
      "[d=3] epoch 33, loss=4.143\n",
      "[d=3] epoch 34, loss=4.128\n",
      "[d=3] epoch 35, loss=4.052\n",
      "[d=3] epoch 36, loss=4.034\n",
      "[d=3] epoch 37, loss=4.073\n",
      "[d=3] epoch 38, loss=4.113\n",
      "[d=3] epoch 39, loss=3.964\n",
      "[d=3] epoch 40, loss=3.918\n",
      "[d=3] epoch 41, loss=3.911\n",
      "[d=3] epoch 42, loss=3.867\n",
      "[d=3] epoch 43, loss=3.900\n",
      "[d=3] epoch 44, loss=3.908\n",
      "[d=3] epoch 45, loss=3.793\n",
      "[d=3] epoch 46, loss=3.885\n",
      "[d=3] epoch 47, loss=3.803\n",
      "[d=3] epoch 48, loss=3.786\n",
      "[d=3] epoch 49, loss=3.815\n",
      "[d=3] epoch 50, loss=3.764\n",
      "[d=3] epoch 51, loss=3.772\n",
      "[d=3] epoch 52, loss=3.762\n",
      "[d=3] epoch 53, loss=3.715\n",
      "[d=3] epoch 54, loss=3.652\n",
      "[d=3] epoch 55, loss=3.694\n",
      "[d=3] epoch 56, loss=3.734\n",
      "[d=3] epoch 57, loss=3.757\n",
      "[d=3] epoch 58, loss=3.639\n",
      "[d=3] epoch 59, loss=3.684\n",
      "[d=3] epoch 60, loss=3.672\n",
      "[d=3] epoch 61, loss=3.649\n",
      "[d=3] epoch 62, loss=3.663\n",
      "[d=3] epoch 63, loss=3.613\n",
      "[d=3] epoch 64, loss=3.640\n",
      "[d=3] epoch 65, loss=3.544\n",
      "[d=3] epoch 66, loss=3.609\n",
      "[d=3] epoch 67, loss=3.567\n",
      "[d=3] epoch 68, loss=3.607\n",
      "[d=3] epoch 69, loss=3.624\n",
      "[d=3] epoch 70, loss=3.685\n",
      "[d=3] epoch 71, loss=3.623\n",
      "[d=3] epoch 72, loss=3.565\n",
      "[d=3] epoch 73, loss=3.517\n",
      "[d=3] epoch 74, loss=3.553\n",
      "[d=3] epoch 75, loss=3.527\n",
      "[d=3] epoch 76, loss=3.497\n",
      "[d=3] epoch 77, loss=3.549\n",
      "[d=3] epoch 78, loss=3.529\n",
      "[d=3] epoch 79, loss=3.526\n",
      "[d=3] epoch 80, loss=3.524\n",
      "[d=3] epoch 81, loss=3.500\n",
      "[d=3] epoch 82, loss=3.502\n",
      "[d=3] epoch 83, loss=3.523\n",
      "[d=3] epoch 84, loss=3.446\n",
      "[d=3] epoch 85, loss=3.601\n",
      "[d=3] epoch 86, loss=3.515\n",
      "[d=3] epoch 87, loss=3.573\n",
      "[d=3] epoch 88, loss=3.444\n",
      "[d=3] epoch 89, loss=3.458\n",
      "[d=3] epoch 90, loss=3.489\n",
      "[d=3] epoch 91, loss=3.496\n",
      "[d=3] epoch 92, loss=3.452\n",
      "[d=3] epoch 93, loss=3.468\n",
      "[d=3] epoch 94, loss=3.451\n",
      "[d=3] epoch 95, loss=3.463\n",
      "[d=3] epoch 96, loss=3.389\n",
      "[d=3] epoch 97, loss=3.464\n",
      "[d=3] epoch 98, loss=3.393\n",
      "[d=3] epoch 99, loss=3.365\n",
      "[d=3] epoch 100, loss=3.389\n",
      "[d=3] FNN bit accuracy: X=0.8944, Z=0.9078\n",
      "\n",
      "=========================================\n",
      "Training distance 5\n",
      "=========================================\n",
      "[d=5] epoch 1, loss=19.157\n",
      "[d=5] epoch 2, loss=15.065\n",
      "[d=5] epoch 3, loss=11.423\n",
      "[d=5] epoch 4, loss=9.439\n",
      "[d=5] epoch 5, loss=8.306\n",
      "[d=5] epoch 6, loss=7.672\n",
      "[d=5] epoch 7, loss=7.273\n",
      "[d=5] epoch 8, loss=6.939\n",
      "[d=5] epoch 9, loss=6.663\n",
      "[d=5] epoch 10, loss=6.357\n",
      "[d=5] epoch 11, loss=6.089\n",
      "[d=5] epoch 12, loss=5.907\n",
      "[d=5] epoch 13, loss=5.668\n",
      "[d=5] epoch 14, loss=5.391\n",
      "[d=5] epoch 15, loss=5.189\n",
      "[d=5] epoch 16, loss=4.990\n",
      "[d=5] epoch 17, loss=4.839\n",
      "[d=5] epoch 18, loss=4.627\n",
      "[d=5] epoch 19, loss=4.496\n",
      "[d=5] epoch 20, loss=4.271\n",
      "[d=5] epoch 21, loss=4.141\n",
      "[d=5] epoch 22, loss=3.992\n",
      "[d=5] epoch 23, loss=3.863\n",
      "[d=5] epoch 24, loss=3.677\n",
      "[d=5] epoch 25, loss=3.585\n",
      "[d=5] epoch 26, loss=3.459\n",
      "[d=5] epoch 27, loss=3.299\n",
      "[d=5] epoch 28, loss=3.215\n",
      "[d=5] epoch 29, loss=3.097\n",
      "[d=5] epoch 30, loss=2.983\n",
      "[d=5] epoch 31, loss=2.878\n",
      "[d=5] epoch 32, loss=2.785\n",
      "[d=5] epoch 33, loss=2.696\n",
      "[d=5] epoch 34, loss=2.618\n",
      "[d=5] epoch 35, loss=2.523\n",
      "[d=5] epoch 36, loss=2.459\n",
      "[d=5] epoch 37, loss=2.372\n",
      "[d=5] epoch 38, loss=2.306\n",
      "[d=5] epoch 39, loss=2.221\n",
      "[d=5] epoch 40, loss=2.165\n",
      "[d=5] epoch 41, loss=2.094\n",
      "[d=5] epoch 42, loss=2.041\n",
      "[d=5] epoch 43, loss=1.973\n",
      "[d=5] epoch 44, loss=1.915\n",
      "[d=5] epoch 45, loss=1.850\n",
      "[d=5] epoch 46, loss=1.782\n",
      "[d=5] epoch 47, loss=1.733\n",
      "[d=5] epoch 48, loss=1.677\n",
      "[d=5] epoch 49, loss=1.625\n",
      "[d=5] epoch 50, loss=1.583\n",
      "[d=5] epoch 51, loss=1.532\n",
      "[d=5] epoch 52, loss=1.484\n",
      "[d=5] epoch 53, loss=1.441\n",
      "[d=5] epoch 54, loss=1.401\n",
      "[d=5] epoch 55, loss=1.365\n",
      "[d=5] epoch 56, loss=1.317\n",
      "[d=5] epoch 57, loss=1.279\n",
      "[d=5] epoch 58, loss=1.234\n",
      "[d=5] epoch 59, loss=1.194\n",
      "[d=5] epoch 60, loss=1.159\n",
      "[d=5] epoch 61, loss=1.139\n",
      "[d=5] epoch 62, loss=1.108\n",
      "[d=5] epoch 63, loss=1.062\n",
      "[d=5] epoch 64, loss=1.044\n",
      "[d=5] epoch 65, loss=1.010\n",
      "[d=5] epoch 66, loss=0.986\n",
      "[d=5] epoch 67, loss=0.959\n",
      "[d=5] epoch 68, loss=0.926\n",
      "[d=5] epoch 69, loss=0.899\n",
      "[d=5] epoch 70, loss=0.876\n",
      "[d=5] epoch 71, loss=0.856\n",
      "[d=5] epoch 72, loss=0.828\n",
      "[d=5] epoch 73, loss=0.811\n",
      "[d=5] epoch 74, loss=0.794\n",
      "[d=5] epoch 75, loss=0.781\n",
      "[d=5] epoch 76, loss=0.753\n",
      "[d=5] epoch 77, loss=0.734\n",
      "[d=5] epoch 78, loss=0.710\n",
      "[d=5] epoch 79, loss=0.697\n",
      "[d=5] epoch 80, loss=0.686\n",
      "[d=5] epoch 81, loss=0.667\n",
      "[d=5] epoch 82, loss=0.651\n",
      "[d=5] epoch 83, loss=0.635\n",
      "[d=5] epoch 84, loss=0.621\n",
      "[d=5] epoch 85, loss=0.607\n",
      "[d=5] epoch 86, loss=0.601\n",
      "[d=5] epoch 87, loss=0.587\n",
      "[d=5] epoch 88, loss=0.570\n",
      "[d=5] epoch 89, loss=0.554\n",
      "[d=5] epoch 90, loss=0.550\n",
      "[d=5] epoch 91, loss=0.536\n",
      "[d=5] epoch 92, loss=0.526\n",
      "[d=5] epoch 93, loss=0.513\n",
      "[d=5] epoch 94, loss=0.497\n",
      "[d=5] epoch 95, loss=0.495\n",
      "[d=5] epoch 96, loss=0.477\n",
      "[d=5] epoch 97, loss=0.474\n",
      "[d=5] epoch 98, loss=0.457\n",
      "[d=5] epoch 99, loss=0.446\n",
      "[d=5] epoch 100, loss=0.443\n",
      "[d=5] FNN bit accuracy: X=0.9468, Z=0.9552\n",
      "\n",
      "=========================================\n",
      "Training distance 7\n",
      "=========================================\n",
      "[d=7] epoch 1, loss=20.306\n",
      "[d=7] epoch 2, loss=17.805\n",
      "[d=7] epoch 3, loss=14.709\n",
      "[d=7] epoch 4, loss=12.102\n",
      "[d=7] epoch 5, loss=10.246\n",
      "[d=7] epoch 6, loss=9.165\n",
      "[d=7] epoch 7, loss=8.413\n",
      "[d=7] epoch 8, loss=7.847\n",
      "[d=7] epoch 9, loss=7.361\n",
      "[d=7] epoch 10, loss=6.986\n",
      "[d=7] epoch 11, loss=6.717\n",
      "[d=7] epoch 12, loss=6.397\n",
      "[d=7] epoch 13, loss=6.147\n",
      "[d=7] epoch 14, loss=5.966\n",
      "[d=7] epoch 15, loss=5.792\n",
      "[d=7] epoch 16, loss=5.669\n",
      "[d=7] epoch 17, loss=5.389\n",
      "[d=7] epoch 18, loss=5.231\n",
      "[d=7] epoch 19, loss=5.104\n",
      "[d=7] epoch 20, loss=4.958\n",
      "[d=7] epoch 21, loss=4.813\n",
      "[d=7] epoch 22, loss=4.655\n",
      "[d=7] epoch 23, loss=4.505\n",
      "[d=7] epoch 24, loss=4.358\n",
      "[d=7] epoch 25, loss=4.235\n",
      "[d=7] epoch 26, loss=4.105\n",
      "[d=7] epoch 27, loss=3.983\n",
      "[d=7] epoch 28, loss=3.878\n",
      "[d=7] epoch 29, loss=3.753\n",
      "[d=7] epoch 30, loss=3.613\n",
      "[d=7] epoch 31, loss=3.520\n",
      "[d=7] epoch 32, loss=3.389\n",
      "[d=7] epoch 33, loss=3.310\n",
      "[d=7] epoch 34, loss=3.206\n",
      "[d=7] epoch 35, loss=3.109\n",
      "[d=7] epoch 36, loss=2.995\n",
      "[d=7] epoch 37, loss=2.906\n",
      "[d=7] epoch 38, loss=2.835\n",
      "[d=7] epoch 39, loss=2.759\n",
      "[d=7] epoch 40, loss=2.664\n",
      "[d=7] epoch 41, loss=2.581\n",
      "[d=7] epoch 42, loss=2.491\n",
      "[d=7] epoch 43, loss=2.423\n",
      "[d=7] epoch 44, loss=2.363\n",
      "[d=7] epoch 45, loss=2.278\n",
      "[d=7] epoch 46, loss=2.205\n",
      "[d=7] epoch 47, loss=2.140\n",
      "[d=7] epoch 48, loss=2.053\n",
      "[d=7] epoch 49, loss=2.002\n",
      "[d=7] epoch 50, loss=1.940\n",
      "[d=7] epoch 51, loss=1.869\n",
      "[d=7] epoch 52, loss=1.808\n",
      "[d=7] epoch 53, loss=1.761\n",
      "[d=7] epoch 54, loss=1.700\n",
      "[d=7] epoch 55, loss=1.630\n",
      "[d=7] epoch 56, loss=1.583\n",
      "[d=7] epoch 57, loss=1.525\n",
      "[d=7] epoch 58, loss=1.478\n",
      "[d=7] epoch 59, loss=1.438\n",
      "[d=7] epoch 60, loss=1.389\n",
      "[d=7] epoch 61, loss=1.334\n",
      "[d=7] epoch 62, loss=1.296\n",
      "[d=7] epoch 63, loss=1.246\n",
      "[d=7] epoch 64, loss=1.205\n",
      "[d=7] epoch 65, loss=1.154\n",
      "[d=7] epoch 66, loss=1.120\n",
      "[d=7] epoch 67, loss=1.073\n",
      "[d=7] epoch 68, loss=1.038\n",
      "[d=7] epoch 69, loss=1.006\n",
      "[d=7] epoch 70, loss=0.970\n",
      "[d=7] epoch 71, loss=0.943\n",
      "[d=7] epoch 72, loss=0.910\n",
      "[d=7] epoch 73, loss=0.875\n",
      "[d=7] epoch 74, loss=0.844\n",
      "[d=7] epoch 75, loss=0.814\n",
      "[d=7] epoch 76, loss=0.787\n",
      "[d=7] epoch 77, loss=0.759\n",
      "[d=7] epoch 78, loss=0.730\n",
      "[d=7] epoch 79, loss=0.704\n",
      "[d=7] epoch 80, loss=0.677\n",
      "[d=7] epoch 81, loss=0.650\n",
      "[d=7] epoch 82, loss=0.627\n",
      "[d=7] epoch 83, loss=0.604\n",
      "[d=7] epoch 84, loss=0.583\n",
      "[d=7] epoch 85, loss=0.566\n",
      "[d=7] epoch 86, loss=0.542\n",
      "[d=7] epoch 87, loss=0.520\n",
      "[d=7] epoch 88, loss=0.500\n",
      "[d=7] epoch 89, loss=0.486\n",
      "[d=7] epoch 90, loss=0.468\n",
      "[d=7] epoch 91, loss=0.450\n",
      "[d=7] epoch 92, loss=0.434\n",
      "[d=7] epoch 93, loss=0.419\n",
      "[d=7] epoch 94, loss=0.406\n",
      "[d=7] epoch 95, loss=0.389\n",
      "[d=7] epoch 96, loss=0.378\n",
      "[d=7] epoch 97, loss=0.362\n",
      "[d=7] epoch 98, loss=0.351\n",
      "[d=7] epoch 99, loss=0.340\n",
      "[d=7] epoch 100, loss=0.330\n",
      "[d=7] FNN bit accuracy: X=0.9492, Z=0.9463\n",
      "\n",
      "=========================================\n",
      "Training distance 9\n",
      "=========================================\n",
      "[d=9] epoch 1, loss=20.393\n",
      "[d=9] epoch 2, loss=18.643\n",
      "[d=9] epoch 3, loss=16.164\n",
      "[d=9] epoch 4, loss=13.552\n",
      "[d=9] epoch 5, loss=11.818\n",
      "[d=9] epoch 6, loss=10.585\n",
      "[d=9] epoch 7, loss=9.558\n",
      "[d=9] epoch 8, loss=8.819\n",
      "[d=9] epoch 9, loss=8.240\n",
      "[d=9] epoch 10, loss=7.752\n",
      "[d=9] epoch 11, loss=7.411\n",
      "[d=9] epoch 12, loss=7.072\n",
      "[d=9] epoch 13, loss=6.710\n",
      "[d=9] epoch 14, loss=6.439\n",
      "[d=9] epoch 15, loss=6.196\n",
      "[d=9] epoch 16, loss=5.939\n",
      "[d=9] epoch 17, loss=5.736\n",
      "[d=9] epoch 18, loss=5.590\n",
      "[d=9] epoch 19, loss=5.412\n",
      "[d=9] epoch 20, loss=5.186\n",
      "[d=9] epoch 21, loss=5.043\n",
      "[d=9] epoch 22, loss=4.915\n",
      "[d=9] epoch 23, loss=4.771\n",
      "[d=9] epoch 24, loss=4.647\n",
      "[d=9] epoch 25, loss=4.483\n",
      "[d=9] epoch 26, loss=4.316\n",
      "[d=9] epoch 27, loss=4.215\n",
      "[d=9] epoch 28, loss=4.107\n",
      "[d=9] epoch 29, loss=3.993\n",
      "[d=9] epoch 30, loss=3.874\n",
      "[d=9] epoch 31, loss=3.779\n",
      "[d=9] epoch 32, loss=3.683\n",
      "[d=9] epoch 33, loss=3.545\n",
      "[d=9] epoch 34, loss=3.446\n",
      "[d=9] epoch 35, loss=3.335\n",
      "[d=9] epoch 36, loss=3.259\n",
      "[d=9] epoch 37, loss=3.167\n",
      "[d=9] epoch 38, loss=3.066\n",
      "[d=9] epoch 39, loss=2.975\n",
      "[d=9] epoch 40, loss=2.905\n",
      "[d=9] epoch 41, loss=2.825\n",
      "[d=9] epoch 42, loss=2.749\n",
      "[d=9] epoch 43, loss=2.673\n",
      "[d=9] epoch 44, loss=2.606\n",
      "[d=9] epoch 45, loss=2.526\n",
      "[d=9] epoch 46, loss=2.448\n",
      "[d=9] epoch 47, loss=2.369\n",
      "[d=9] epoch 48, loss=2.303\n",
      "[d=9] epoch 49, loss=2.229\n",
      "[d=9] epoch 50, loss=2.176\n",
      "[d=9] epoch 51, loss=2.108\n",
      "[d=9] epoch 52, loss=2.044\n",
      "[d=9] epoch 53, loss=2.002\n",
      "[d=9] epoch 54, loss=1.935\n",
      "[d=9] epoch 55, loss=1.865\n",
      "[d=9] epoch 56, loss=1.812\n",
      "[d=9] epoch 57, loss=1.752\n",
      "[d=9] epoch 58, loss=1.698\n",
      "[d=9] epoch 59, loss=1.643\n",
      "[d=9] epoch 60, loss=1.591\n",
      "[d=9] epoch 61, loss=1.540\n",
      "[d=9] epoch 62, loss=1.493\n",
      "[d=9] epoch 63, loss=1.435\n",
      "[d=9] epoch 64, loss=1.390\n",
      "[d=9] epoch 65, loss=1.343\n",
      "[d=9] epoch 66, loss=1.294\n",
      "[d=9] epoch 67, loss=1.251\n",
      "[d=9] epoch 68, loss=1.219\n",
      "[d=9] epoch 69, loss=1.181\n",
      "[d=9] epoch 70, loss=1.146\n",
      "[d=9] epoch 71, loss=1.105\n",
      "[d=9] epoch 72, loss=1.066\n",
      "[d=9] epoch 73, loss=1.033\n",
      "[d=9] epoch 74, loss=0.999\n",
      "[d=9] epoch 75, loss=0.963\n",
      "[d=9] epoch 76, loss=0.935\n",
      "[d=9] epoch 77, loss=0.900\n",
      "[d=9] epoch 78, loss=0.870\n",
      "[d=9] epoch 79, loss=0.837\n",
      "[d=9] epoch 80, loss=0.808\n",
      "[d=9] epoch 81, loss=0.786\n",
      "[d=9] epoch 82, loss=0.760\n",
      "[d=9] epoch 83, loss=0.727\n",
      "[d=9] epoch 84, loss=0.703\n",
      "[d=9] epoch 85, loss=0.681\n",
      "[d=9] epoch 86, loss=0.655\n",
      "[d=9] epoch 87, loss=0.631\n",
      "[d=9] epoch 88, loss=0.609\n",
      "[d=9] epoch 89, loss=0.582\n",
      "[d=9] epoch 90, loss=0.564\n",
      "[d=9] epoch 91, loss=0.545\n",
      "[d=9] epoch 92, loss=0.523\n",
      "[d=9] epoch 93, loss=0.504\n",
      "[d=9] epoch 94, loss=0.486\n",
      "[d=9] epoch 95, loss=0.468\n",
      "[d=9] epoch 96, loss=0.452\n",
      "[d=9] epoch 97, loss=0.437\n",
      "[d=9] epoch 98, loss=0.422\n",
      "[d=9] epoch 99, loss=0.407\n",
      "[d=9] epoch 100, loss=0.392\n",
      "[d=9] FNN bit accuracy: X=0.9326, Z=0.9286\n",
      "\n",
      "=========================================\n",
      "Training distance 11\n",
      "=========================================\n",
      "[d=11] epoch 1, loss=20.574\n",
      "[d=11] epoch 2, loss=19.408\n",
      "[d=11] epoch 3, loss=17.580\n",
      "[d=11] epoch 4, loss=15.289\n",
      "[d=11] epoch 5, loss=13.327\n",
      "[d=11] epoch 6, loss=11.871\n",
      "[d=11] epoch 7, loss=10.730\n",
      "[d=11] epoch 8, loss=9.893\n",
      "[d=11] epoch 9, loss=9.175\n",
      "[d=11] epoch 10, loss=8.623\n",
      "[d=11] epoch 11, loss=8.038\n",
      "[d=11] epoch 12, loss=7.576\n",
      "[d=11] epoch 13, loss=7.190\n",
      "[d=11] epoch 14, loss=6.814\n",
      "[d=11] epoch 15, loss=6.539\n",
      "[d=11] epoch 16, loss=6.257\n",
      "[d=11] epoch 17, loss=5.957\n",
      "[d=11] epoch 18, loss=5.734\n",
      "[d=11] epoch 19, loss=5.542\n",
      "[d=11] epoch 20, loss=5.385\n",
      "[d=11] epoch 21, loss=5.141\n",
      "[d=11] epoch 22, loss=4.943\n",
      "[d=11] epoch 23, loss=4.724\n",
      "[d=11] epoch 24, loss=4.587\n",
      "[d=11] epoch 25, loss=4.428\n",
      "[d=11] epoch 26, loss=4.278\n",
      "[d=11] epoch 27, loss=4.180\n",
      "[d=11] epoch 28, loss=4.043\n",
      "[d=11] epoch 29, loss=3.883\n",
      "[d=11] epoch 30, loss=3.783\n",
      "[d=11] epoch 31, loss=3.658\n",
      "[d=11] epoch 32, loss=3.543\n",
      "[d=11] epoch 33, loss=3.444\n",
      "[d=11] epoch 34, loss=3.315\n",
      "[d=11] epoch 35, loss=3.270\n",
      "[d=11] epoch 36, loss=3.164\n",
      "[d=11] epoch 37, loss=3.050\n",
      "[d=11] epoch 38, loss=2.956\n",
      "[d=11] epoch 39, loss=2.865\n",
      "[d=11] epoch 40, loss=2.795\n",
      "[d=11] epoch 41, loss=2.710\n",
      "[d=11] epoch 42, loss=2.639\n",
      "[d=11] epoch 43, loss=2.567\n",
      "[d=11] epoch 44, loss=2.496\n",
      "[d=11] epoch 45, loss=2.403\n",
      "[d=11] epoch 46, loss=2.333\n",
      "[d=11] epoch 47, loss=2.260\n",
      "[d=11] epoch 48, loss=2.193\n",
      "[d=11] epoch 49, loss=2.130\n",
      "[d=11] epoch 50, loss=2.071\n",
      "[d=11] epoch 51, loss=2.000\n",
      "[d=11] epoch 52, loss=1.947\n",
      "[d=11] epoch 53, loss=1.883\n",
      "[d=11] epoch 54, loss=1.823\n",
      "[d=11] epoch 55, loss=1.784\n",
      "[d=11] epoch 56, loss=1.727\n",
      "[d=11] epoch 57, loss=1.679\n",
      "[d=11] epoch 58, loss=1.614\n",
      "[d=11] epoch 59, loss=1.570\n",
      "[d=11] epoch 60, loss=1.520\n",
      "[d=11] epoch 61, loss=1.477\n",
      "[d=11] epoch 62, loss=1.431\n",
      "[d=11] epoch 63, loss=1.381\n",
      "[d=11] epoch 64, loss=1.336\n",
      "[d=11] epoch 65, loss=1.290\n",
      "[d=11] epoch 66, loss=1.246\n",
      "[d=11] epoch 67, loss=1.208\n",
      "[d=11] epoch 68, loss=1.167\n",
      "[d=11] epoch 69, loss=1.122\n",
      "[d=11] epoch 70, loss=1.090\n",
      "[d=11] epoch 71, loss=1.054\n",
      "[d=11] epoch 72, loss=1.019\n",
      "[d=11] epoch 73, loss=0.983\n",
      "[d=11] epoch 74, loss=0.949\n",
      "[d=11] epoch 75, loss=0.918\n",
      "[d=11] epoch 76, loss=0.889\n",
      "[d=11] epoch 77, loss=0.857\n",
      "[d=11] epoch 78, loss=0.828\n",
      "[d=11] epoch 79, loss=0.799\n",
      "[d=11] epoch 80, loss=0.774\n",
      "[d=11] epoch 81, loss=0.750\n",
      "[d=11] epoch 82, loss=0.723\n",
      "[d=11] epoch 83, loss=0.695\n",
      "[d=11] epoch 84, loss=0.671\n",
      "[d=11] epoch 85, loss=0.649\n",
      "[d=11] epoch 86, loss=0.625\n",
      "[d=11] epoch 87, loss=0.607\n",
      "[d=11] epoch 88, loss=0.583\n",
      "[d=11] epoch 89, loss=0.561\n",
      "[d=11] epoch 90, loss=0.544\n",
      "[d=11] epoch 91, loss=0.527\n",
      "[d=11] epoch 92, loss=0.507\n",
      "[d=11] epoch 93, loss=0.489\n",
      "[d=11] epoch 94, loss=0.475\n",
      "[d=11] epoch 95, loss=0.456\n",
      "[d=11] epoch 96, loss=0.439\n",
      "[d=11] epoch 97, loss=0.424\n",
      "[d=11] epoch 98, loss=0.409\n",
      "[d=11] epoch 99, loss=0.395\n",
      "[d=11] epoch 100, loss=0.380\n",
      "[d=11] FNN bit accuracy: X=0.9311, Z=0.9312\n"
     ]
    }
   ],
   "source": [
    "for d in distances:\n",
    "    dist = d\n",
    "    print(\"\\n=========================================\")\n",
    "    print(\"Training distance\", d)\n",
    "    print(\"=========================================\")\n",
    "\n",
    "    train_fnn_for_distance(dist,\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1f300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra-m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
